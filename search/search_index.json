{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#gpt-rag-solution-accelerator","title":"GPT-RAG Solution Accelerator","text":""},{"location":"#overview","title":"Overview","text":"<p>GPT-RAG is a powerful, enterprise-ready accelerator that transforms how organizations build Retrieval-Augmented Generation (RAG) solutions on Azure. By leveraging Azure OpenAI, AI Search, and AI Foundry, it delivers a secure, modular foundation for creating intelligent conversational assistants and data-driven applications. View the source code on GitHub.</p> <p></p> <p>Built from the ground up with Zero-Trust security principles and Infrastructure as Code (IaC), GPT-RAG accelerates your journey to production-ready AI solutions. Whether you're working with text, images, or voice, this comprehensive platform combines enterprise-grade security, intelligent orchestration, and multimodal capabilities to deliver exceptional user experiences faster than ever.</p>"},{"location":"#core-services","title":"Core Services","text":"Services Description Orchestrator Manages multi-agent workflows and retrieves context using Semantic Kernel and Azure AI. Web UI User interface for chat interactions, supports streaming and custom themes. Data Ingestion Extracts, chunks, and indexes enterprise data for optimized retrieval. MCP Server Implements the Model Context Protocol for tool hosting and business logic integration."},{"location":"#contributing","title":"Contributing","text":"<p>We welcome contributions from the community! Check our Contribution Guidelines for CLA, code of conduct, and PR guidelines.</p>"},{"location":"architecture/","title":"Overview","text":""},{"location":"architecture/#architecture","title":"\ud83c\udfdb\ufe0f Architecture","text":"<p>Download Visio Diagram</p>"},{"location":"architecture/#key-capabilities","title":"Key Capabilities","text":"<ul> <li> <p>Enterprise-Grade Security   Zero-Trust architecture with private endpoints, Azure Key Vault integration, and comprehensive monitoring.</p> </li> <li> <p>Flexible &amp; Customizable   Modular design with customizable orchestration, multiple interface options, and bring-your-own-resources support.</p> </li> <li> <p>Multimodal Experience   Native support for text, images, and voice with SharePoint and Fabric connectors for seamless data integration.</p> </li> <li> <p>Production Ready   Enterprise-ready infrastructure with support for CI/CD pipelines and quality evaluation integration.</p> </li> </ul>"},{"location":"contributing/","title":"Contributing","text":"<p>We appreciate contributions and suggestions for this project! Before contributing, you'll need to sign a Contributor License Agreement (CLA) to confirm that you have the rights to, and do, grant us permission to use your contribution. More details can be found at Microsoft CLA.</p> <p>This project adheres to the Microsoft Open Source Code of Conduct. For more information, please visit the Code of Conduct FAQ or contact opencode@microsoft.com with any questions or comments.</p> <p>Below, you'll find details on how our code update process works and instructions on how to contribute.</p>"},{"location":"contributing/#contributors","title":"\ud83e\udd1d Contributors","text":"<p>Contributions include code, feedback, design ideas, documentation, and collaboration.</p>"},{"location":"contributing/#ways-to-contribute","title":"Ways to Contribute","text":"<ul> <li>Issues: Report bugs, propose enhancements, or share feature requests.</li> <li>Comments: Engage in discussions, help others, and review proposals.</li> <li>Documentation: Improve guides and clarity for new users.</li> <li>Design: Contribute to open design discussions and new patterns.</li> <li>Tests: Strengthen reliability through unit and integration tests.</li> <li>Code: Submit fixes, enhancements, or new modules via pull requests.</li> </ul>"},{"location":"contributing/#contribution-guidelines","title":"Contribution Guidelines","text":"<p>To maintain project quality, the following items will be considered during the PR review.</p> <p>Adhering to these best practices will streamline the review process.</p> <ul> <li> <p>Target the <code>develop</code> Branch: Always direct your pull request to the <code>develop</code> branch to ensure that changes are properly integrated into the project's development workflow.</p> </li> <li> <p>Keep Pull Requests Small: Aim to make your pull requests as focused and concise as possible. This makes it easier to review and ensures quicker integration into the codebase.</p> </li> <li> <p>Associate with Prioritized Issues: Ensure that each pull request is linked to a specific, prioritized issue in the project backlog. This helps maintain alignment with project goals and ensures that work is being done on tasks of the highest importance.</p> </li> <li> <p>Include Documentation: Every new feature or functionality must be accompanied by clear documentation explaining its purpose and configuration. This ensures others can use it independently in a self-service manner.</p> </li> <li> <p>Bugs and Documentation Corrections: Pull requests that address bugs or correct documentation do not need to be associated with prioritized issues. These can be submitted directly to maintain the quality and accuracy of the project.</p> </li> <li> <p>Multi-Repo Dependencies: If your pull request has dependencies on updates in other repositories, make sure to mention this in the pull request description. Additionally, create a corresponding pull request in the other repository to ensure synchronized updates across all related projects.</p> </li> </ul>"},{"location":"contributing/#code-update-workflow","title":"Code Update Workflow","text":"<p>We use a simplified version of the Fork and Branch Workflow alongside Git Flow for branching strategy. The <code>main</code> branch always contains deployment-ready code, while the <code>develop</code> branch serves as our integration branch.</p> <p>Contributors create feature branches from <code>develop</code> in their forks. Once changes are completed, they submit a pull request to the <code>develop</code> branch in the upstream repository. After review and approval, reviewers merge the changes into <code>develop</code>. Weekly, maintainers group these changes into a pull request from <code>develop</code> to <code>main</code> for final review and merging.</p>"},{"location":"contributing/#process-overview","title":"Process Overview","text":"<p>This section outlines the contribution process, highlighting the key actions for both contributors and maintainers. The accompanying diagram visually represents the workflow.</p> <p></p> <p>1) Fork the Repository</p> <p>Create a copy of the GPT-RAG upstream repository under your own GitHub account.</p> <p>2) Clone Locally</p> <p>Download your forked repository to your local machine.</p> <p>3) Add Upstream </p> <p>Link the original GPT-RAG upstream repository as <code>upstream</code> to keep your fork synchronized.</p> <p>4) Create a Feature Branch</p> <p>From your fork\u2019s <code>develop</code> branch, create a feature branch for your change (e.g., <code>feature/feature_x</code>).</p> <p>5) Commit and Push Changes</p> <p>Implement your updates locally, commit, and push them to your fork on GitHub.</p> <p>6) Open and Merge the Pull Request to <code>develop</code></p> <p>Open a PR from your feature branch in your fork to the upstream repository\u2019s <code>develop</code> branch.</p> <p>7) Sync with Upstream <code>develop</code></p> <p>After your PR is merged, update your fork's <code>develop</code> branch with the latest changes from the upstream.</p> <p>8) Create a Release Branch (Maintainers)</p> <p>When the <code>develop</code> branch is ready for release, create a branch named <code>release/x.y.z</code> from your fork's <code>develop</code>. This branch will be tested and validated before merging to <code>main</code>.</p> <p>9) Open a Pull Request to Upstream <code>main</code> (Maintainers)</p> <p>Once the release is validated, open a PR from your release branch to the upstream <code>main</code>. After the merge, maintainers will create a version tag (e.g., <code>v2.0.1</code>).</p> <p>10) Sync Your Fork</p> <p>Finally, update both your fork\u2019s <code>main</code> and <code>develop</code> branches to reflect the latest upstream state.</p>"},{"location":"contributing/#step-by-step","title":"Step-by-Step","text":"<p>Here\u2019s an example of implementing a feature called <code>conversation-metadata</code> in the <code>gpt-rag-orchestrator</code> repository.</p> <p>1) Create a Fork</p> <p><code>bash    https://github.com/placerda/gpt-rag-orchestrator.git</code></p> <p>2) Clone Your Fork Locally</p> <p><code>bash    git clone https://github.com/placerda/gpt-rag-orchestrator.git</code></p> <p>3) Set Upstream Remote</p> <p><code>bash    git remote add upstream git@github.com:Azure/gpt-rag-orchestrator.git</code></p> <p>4) Create a Feature Branch</p> <p><code>bash    git checkout -b feature/conversation-metadata develop</code></p> <p>5) Make and Push Your Changes</p> <p><code>bash    git add .    git commit -m \"Implemented conversation metadata\"    git push origin feature/conversation-metadata</code></p> <p>6) Open and Merge the Pull Request to <code>develop</code></p> <ul> <li>6a. Create the PR:      Go to your fork on GitHub \u2192 click New Pull Request \u2192      Base: <code>Azure/gpt-rag-orchestrator</code> \u2192 <code>develop</code>      Compare: <code>placerda/gpt-rag-orchestrator</code> \u2192 <code>feature/conversation-metadata</code></li> <li>6b. Maintainer Review:      The maintainers will review, request changes if needed, and merge the PR into the upstream <code>develop</code>.</li> </ul> <p>7) Sync Your Fork\u2019s <code>develop</code></p> <p><code>bash    git fetch upstream    git checkout develop    git merge upstream/develop    git push origin develop</code></p> <p>8) Create a Release Branch (Maintainers)</p> <p><code>bash    git checkout -b release/2.0.1 develop    git push origin release/2.0.1</code></p> <p>9) Open a Pull Request to Upstream <code>main</code> (Maintainers)</p> <ul> <li>Base: <code>Azure/gpt-rag-orchestrator</code> \u2192 <code>main</code></li> <li>Compare: <code>placerda/gpt-rag-orchestrator</code> \u2192 <code>release/2.0.1</code></li> <li>After review and merge, maintainers tag the release (e.g., <code>v2.0.1</code>).</li> </ul> <p>10) Sync Your Fork After Tag Creation</p> <pre><code>git fetch upstream\ngit checkout main\ngit merge upstream/main\ngit push origin main\n</code></pre>"},{"location":"deploy/","title":"\ud83d\ude80 Deployment Guide","text":"<p>Choose your preferred deployment method based on project requirements and environment constraints.</p> <p>Note: You can change parameter values in <code>main.parameters.json</code> or set them with <code>azd env set</code> before running <code>azd provision</code>. This applies only to parameters that support environment variable substitution.</p>"},{"location":"deploy/#prerequisites","title":"Prerequisites","text":"<p>Required Permissions:</p> <ul> <li>Azure subscription with Contributor and User Access Admin roles</li> <li>Agreement to Responsible AI terms for Azure AI Services</li> </ul> <p>Required Tools:</p> <ul> <li>Azure Developer CLI</li> <li>PowerShell 7+ (Windows only)</li> <li>Git</li> <li>Python 3.12</li> </ul>"},{"location":"deploy/#quick-start-basic-deployment","title":"Quick Start - Basic Deployment","text":"<p>Quick setup for demos without network isolation.</p> <pre><code>azd init -t azure/gpt-rag\nazd provision\n</code></pre> <p>Demo video:</p>"},{"location":"deploy/#zero-trust-deployment","title":"Zero Trust Deployment","text":"<p>For deployments that require network isolation.</p>"},{"location":"deploy/#before-provisioning","title":"Before Provisioning","text":"<p>Enable network isolation in your environment:</p> <pre><code>azd env set NETWORK_ISOLATION true\n</code></pre>"},{"location":"deploy/#provision-infrastructure","title":"Provision Infrastructure","text":"<pre><code>azd provision\n</code></pre>"},{"location":"deploy/#post-provision-configuration","title":"Post-Provision Configuration","text":"<p>The Bicep template provisions a Jumpbox VM by default. You can connect to it to perform the post-provision steps, deploy services, and run tests.</p> <p>Option A \u2013 Using the deployed Jumpbox VM</p> <ol> <li>Reset the VM password in the Azure Portal (required on first access if not set in deployment parameters): Go to your VM resource \u2192 Support + troubleshooting \u2192 Reset password \u2192 Set new credentials.</li> <li>Connect via Azure Bastion.</li> <li>Open a terminal in the VM and run:</li> </ol> <pre><code>cd c:\\github\\gpt-rag\n.\\scripts\\postProvision.ps1\n</code></pre> <p>Option B \u2013 From your local machine (must have VNet access)</p> <p>From the <code>gpt-rag</code> directory, run:</p> <pre><code>.\\scripts\\postProvision.ps1\n</code></pre> <p>or (Bash)</p> <pre><code>.\\scripts\\postProvision.sh\n</code></pre> <p>Note: If you have re-initialized or cloned the gpt-rag repo again, refresh your <code>azd</code> environment before running postProvision script so it points to the existing deployment: <code>azd init -t azure/gpt-rag</code> then <code>azd env refresh</code>. When prompted, select the same Subscription, Resource Group, and Location as the original provisioning so <code>azd</code> correctly links to your environment.</p>"},{"location":"deploy/#deploy-gpt-rag-services","title":"Deploy GPT-RAG Services","text":"<p>Once the GPT-RAG infrastructure is provisioned, you can deploy the services.</p> <p>To deploy all services at once, navigate to the <code>gpt-rag</code> directory (with azd environment configured) and run:</p> <pre><code>azd deploy\n</code></pre> <p>This command deploys each service in sequence.</p> <p>If you prefer to deploy a single service\u2014for example, when updating only that service\u2014navigate to the corresponding service repository and follow the instructions in its \"How to Deploy\" section.</p>"},{"location":"deploy/#permissions","title":"Permissions","text":"<p>AI Foundry Role and AI Search Assignments</p> Resource Role Assignee Description GenAI App Search Service Search Index Data Reader AI Foundry Project Read index data GenAI App Search Service Search Service Contributor AI Foundry Project Create AI search connection GenAI App Storage Account Storage Blob Data Reader AI Foundry Account Read blob data GenAI App Storage Account Storage Blob Data Reader Search Service Read blob data for search integration AI Foundry Storage Account Storage Blob Data Contributor AI Foundry Project Enable agent to store/retrieve blob artifacts in customer storage AI Foundry Storage Account Containers Storage Blob Data Owner (workspace) AI Foundry Project Scoped owner access to workspace containers for session-specific data AI Foundry Cosmos DB Account Cosmos DB Operator AI Foundry Project Control-plane operations for enterprise memory database (threads) AI Foundry Cosmos DB Containers Cosmos DB Built-in Data Contributor AI Foundry Project Read/write conversation threads within enterprise memory containers AI Foundry Search Service Search Service Contributor AI Foundry Project Create/update indexes for vector search workflows AI Foundry Search Service Search Index Data Contributor AI Foundry Project Read/write index data for embedding-based queries <p>Container App Role Assignments</p> Resource Role Assignee Description GenAI App Configuration Store App Configuration Data Reader ContainerApp: orchestrator Read configuration data GenAI App Configuration Store App Configuration Data Reader ContainerApp: frontend Read configuration data GenAI App Configuration Store App Configuration Data Reader ContainerApp: dataingest Read configuration data GenAI App Configuration Store App Configuration Data Reader ContainerApp: mcp Read configuration data GenAI App Container Registry AcrPull ContainerApp: mcp Pull container images GenAI App Container Registry AcrPull ContainerApp: orchestrator Pull container images GenAI App Container Registry AcrPull ContainerApp: frontend Pull container images GenAI App Container Registry AcrPull ContainerApp: dataingest Pull container images GenAI App Key Vault Key Vault Secrets User ContainerApp: orchestrator Read secrets GenAI App Key Vault Key Vault Secrets User ContainerApp: frontend Read secrets GenAI App Key Vault Key Vault Secrets User ContainerApp: dataingest Read secrets GenAI App Key Vault Key Vault Secrets User ContainerApp: mcp Read secrets GenAI App Search Service Search Index Data Reader ContainerApp: orchestrator Read index data GenAI App Search Service Search Index Data Contributor ContainerApp: dataingest Read/write index data GenAI App Search Service Search Index Data Contributor ContainerApp: mcp Read/write index data GenAI App Storage Account Storage Blob Data Reader ContainerApp: orchestrator Read blob data GenAI App Storage Account Storage Blob Data Reader ContainerApp: frontend Read blob data GenAI App Storage Account Storage Blob Data Contributor ContainerApp: dataingest Read/write blob data GenAI App Storage Account Storage Blob Data Contributor ContainerApp: mcp Read/write blob data GenAI App Storage Account Storage Queue Data Contributor ContainerApp: mcp Read/write storage queue data GenAI App Cosmos DB Cosmos DB Built-in Data Contributor ContainerApp: orchestrator Read/write Cosmos DB data AI Foundry Account Cognitive Services User ContainerApp: orchestrator Access Cognitive Services operations AI Foundry Account Cognitive Services User ContainerApp: dataingest Access Cognitive Services operations AI Foundry Account Cognitive Services OpenAI User ContainerApp: orchestrator Use OpenAI APIs AI Foundry Account Cognitive Services OpenAI User ContainerApp: dataingest Use OpenAI APIs AI Foundry Account Cognitive Services User ContainerApp: mcp Access Cognitive Services AI Foundry Account Cognitive Services OpenAI User ContainerApp: mcp Use OpenAI APIs <p>Executor Role Assignments</p> Resource Role Assignee Description GenAI App Configuration Store App Configuration Data Owner Executor Full control over configuration settings GenAI App Container Registry AcrPush Executor Push container images GenAI App Key Vault Key Vault Contributor Executor Manage Key Vault settings GenAI App Key Vault Key Vault Secrets Officer Executor Create Key Vault secrets GenAI App Search Service Search Service Contributor Executor Create/update search service elements GenAI App Search Service Search Index Data Contributor Executor Read/write search index data GenAI App Storage Account Storage Blob Data Contributor Executor Read/write blob data GenAI App Cosmos DB Cosmos DB Built-in Data Contributor Executor Read/write Cosmos DB data AI Foundry Project Azure AI Project Manager Executor Manage AI Foundry projects and assign roles <p>Jumpbox VM Role Assignments</p> Resource Role Assignee Description GenAI App Container Apps Container Apps Contributor Jumpbox VM Full control over Container Apps (deploy/manage apps) Azure Managed Identity Managed Identity Operator Jumpbox VM Assign and manage user-assigned managed identities GenAI App Container Registry Container Registry Repository Writer Jumpbox VM Write to specific repositories GenAI App Container Registry Container Registry Tasks Contributor Jumpbox VM Manage ACR tasks GenAI App Container Registry Container Registry Data Access Configuration Administrator Jumpbox VM Manage data access configuration for ACR GenAI App Container Registry AcrPush Jumpbox VM Push container images GenAI App Configuration Store App Configuration Data Owner Jumpbox VM Full control over configuration settings GenAI App Key Vault Key Vault Contributor Jumpbox VM Manage Key Vault settings GenAI App Key Vault Key Vault Secrets Officer Jumpbox VM Create Key Vault secrets GenAI App Search Service Search Service Contributor Jumpbox VM Create/update search service elements GenAI App Search Service Search Index Data Contributor Jumpbox VM Read/write search index data GenAI App Storage Account Storage Blob Data Contributor Jumpbox VM Read/write blob data AI Foundry Account Azure AI Project Manager Jumpbox VM Manage AI Foundry projects and assign roles AI Foundry Account Cognitive Services Contributor Jumpbox VM Manage Cognitive Services resources GenAI App Cosmos DB Cosmos DB Built-in Data Contributor Jumpbox VM Read/write Cosmos DB data"},{"location":"ochestrator/","title":"\ud83c\udfaf Orchestrator","text":"<p>The Orchestrator is the core engine of GPT-RAG, managing multi-agent workflows and retrieval operations using Semantic Kernel and Azure AI services.</p>"},{"location":"ochestrator/#key-features","title":"Key Features","text":"<ul> <li>Multi-Agent Workflows: Coordinates multiple AI agents for complex tasks</li> <li>Context Retrieval: Intelligent retrieval from Azure AI Search</li> <li>Semantic Kernel Integration: Built on Microsoft's Semantic Kernel framework</li> <li>Extensible Architecture: Easy to customize and extend</li> </ul>"},{"location":"ochestrator/#getting-started","title":"Getting Started","text":"<p>New to the Orchestrator? Check out our Orchestrator Visual Guide for a visual walkthrough of the architecture and key components.</p>"},{"location":"ochestrator/#repository","title":"Repository","text":"<p>\ud83d\udd17 GitHub Repository</p>"},{"location":"orchestrator-visual-guide/","title":"Orchestrator: Start Your Code Reading with Visuals","text":"<p>A picture is worth a thousand words. Yet many engineers write another thousand words instead of drawing a single useful diagram. Let\u2019s reverse this evolution \u2014 with visuals.</p> <p>Starting with diagrams \u2014 with a bit of simplification and abstraction \u2014 can significantly accelerate the comprehension of complex codebases. This is especially true when the data flow spans multiple execution environments (container app, AI Foundry, Azure cloud resources), where the initial orientation can otherwise be challenging.</p>"},{"location":"orchestrator-visual-guide/#why-this-article-exists","title":"Why This Article Exists","text":"<p>When I started reading the code, I struggled:</p> <ul> <li>Where is the entry point?</li> <li>What calls what?</li> <li>What is the role of the Orchestrator in the data flow?</li> </ul> <p>If you have ever felt dazed and confused by a codebase with many layers of abstraction, this is for you.</p> <p>If you prefer talking for hours about a diagram instead of drawing it, just leave.</p>"},{"location":"orchestrator-visual-guide/#core-architecture-flow","title":"Core Architecture &amp; Flow","text":"<p>The orchestrator's entry point is in src/main.py: <code>orchestrator_endpoint()</code>. In what follows we will consider the Single-Agent RAG Strategy.</p> <pre><code>@app.post(\n    \"/orchestrator\",\n    dependencies=[Depends(validate_auth)], \n    summary=\"Ask orchestrator a question\",\n    response_description=\"Returns the orchestrator\u2019s response in real time, streamed via SSE.\",\n    responses=ORCHESTRATOR_RESPONSES\n)\nasync def orchestrator_endpoint(\n    body: OrchestratorRequest,\n    x_api_key: Optional[str] = Header(None, alias=\"X-API-KEY\"),\n    dapr_api_token: Optional[str] = Header(None, alias=\"dapr-api-token\"),\n):\n</code></pre>"},{"location":"orchestrator-visual-guide/#essential-tasks-of-the-orchestrator","title":"Essential Tasks of the Orchestrator","text":"<p>The <code>Orchestrator</code> class serves as the conversation state manager and strategy coordinator. Its core responsibilities are:</p> <ol> <li>Conversation Lifecycle Management: Creates, loads, and persists conversation documents in the CosmosDB.</li> <li>Strategy Delegation: Routes processing to appropriate agent strategies via factory pattern (<code>AgentStrategyFactory</code>).</li> <li>State Coordination: Ensures conversation state is properly synchronized between database and strategy</li> <li>Response Streaming: Coordinates real-time response delivery while maintaining state consistency</li> </ol>"},{"location":"orchestrator-visual-guide/#single-agent-strategy","title":"Single Agent Strategy","text":"<p>This section explores how the Single-Agent RAG Strategy orchestrates the entire request-response lifecycle, from receiving a user's question to delivering a grounded, streamed answer. The diagrams below illustrate the conversation lifecycle, state management, and the interaction between the Orchestrator container app and AI Foundry services.</p> <p>\ud83d\udfe6 SVG \u00b7 \ud83d\udfe8 PNG \u00b7 \ud83d\udfe5 JPG</p> <p></p> <p>The entry point for the selected Strategy is the method <code>agentic_strategy.initiate_agent_flow()</code>.</p> <p>The strategy object instantiated from the <code>SingleAgentRAGStrategy</code> class runs in the container app and controls the sequence of activities behind the AI Foundry wall. It uses the <code>project_client</code> object as a local proxy (think of it as a remote TV control) to orchestrate operations. The strategy object doesn't handle grounding or LLM calls directly\u2014these are delegated to the AI Foundry agent where the entire RAG pattern is executed.</p> <p>The strategy object <code>SingleAgentRAGStrategy</code></p> <ul> <li> <p>creates a new agent by specifying instructions and a toolbox,</p> </li> <li> <p>retrieves the <code>Thread</code> object based on the thread_id which was retrieved from the CosmosDB,</p> </li> <li> <p>creates a new message from the user's Ask and attaches it to the Thread object,</p> </li> <li> <p>finally calls the project_client.agents.runs.stream() which triggers the RAG pipeline inside of the AI Foundry realm.</p> </li> </ul> <p>Note that <code>Thread</code> objects keep the entire history of conversations. There are two levels of history persistence: one in CosmosDB and another in Thread objects.</p> <p><code>Orchestrator</code> keeps a history (using CosmosDB) identified by <code>conversation_id</code> which arrives in the HTTP Request payload. One of the attributes stored in the CosmosDB is <code>thread_id</code> which points to the <code>Thread</code> object which resides inside of the AI Foundry. AI Foundry maintains its own internal persistency in the Thread objects.</p> <p>The strategy object triggers the RAG pipeline execution inside of AI Foundry with the proxy <code>project_client</code>:</p> <pre><code>project_client.agents.runs.stream(\n    thread_id=thread.id,\n    agent_id=agent.id,\n    ...\n)\n</code></pre> <p>Here is what it does:</p> <ul> <li> <p>Takes the user's Ask.</p> </li> <li> <p>Queries your Azure AI Search index using the <code>AzureAISearchTool</code>.</p> </li> <li> <p>Retrieves relevant document Chunks.</p> </li> <li> <p>Creates the Prompt.</p> </li> <li> <p>Previously retrieved Chunks are included into the Prompt to ground the Response.</p> </li> <li> <p>Prompt is fed into LLM which generates Response.</p> </li> <li> <p>The Response is enhanced by citations and references to the grounding documents.</p> </li> </ul>"},{"location":"orchestrator-visual-guide/#single-agent-strategy-internal-flow","title":"Single Agent Strategy Internal Flow","text":"<p>\ud83d\udfe6 SVG \u00b7 \ud83d\udfe8 PNG \u00b7 \ud83d\udfe5 JPG</p> <p></p> <p>The sequence diagram above is intended to illustrate the core concepts and design patterns present in the codebase. The visualization deliberately simplifies reality through abstraction and by omitting less relevant details.</p>"},{"location":"orchestrator-visual-guide/#links-to-the-code","title":"Links to the code","text":"Concept File Notes Orchestrator entry <code>main.py</code> FastAPI route + request handling Orchestrator implementation <code>orchestration/orchestrator.py</code> Maintains Conversation History + runs streaming pipeline Strategy factory <code>strategies/agent_strategy_factory.py</code> Selects the execution strategy Single-Agent RAG Strategy <code>strategies/single_agent_rag_strategy.py</code> Implements flow to Azure AI Foundry"},{"location":"team/","title":"Meet the Team","text":"Core Team Leads the direction, development, and continuous evolution of GPT-RAG. <sub>Paulo Lacerda</sub> <sub>Project Lead</sub> <sub>Vlad Borys</sub> <sub>Pull Request Reviewer</sub> Engineering Advisor Provides technical guidance and ensures architectural alignment with best practices. <sub>Pablo Castro</sub> <sub>Engineering Advisor</sub> Champions Community advocates helping users succeed, sharing knowledge, and growing the GPT-RAG ecosystem. <sub>Ramesh Jajula</sub> <sub>Vinod Chekkala</sub> <sub>Saurabh Singh</sub> <sub>Varun Nambiar</sub> Founders The visionaries who laid the foundation and shaped GPT-RAG from the ground up. <sub>Paulo Lacerda</sub> <sub>Gonzalo Becerra</sub> <sub>Martin Sciarrilo</sub> Contributors Every contribution matters \u2014 code, documentation, ideas, or feedback. GPT-RAG Orchestrator Ingestion UI MCP <p>Every contribution matters! Whether it's code, docs, ideas, or feedback \u2014 you help make GPT-RAG better.</p> <p>\ud83d\udc49 Learn how to contribute</p>"},{"location":"userfeedback/","title":"User Feedback Configuration","text":"<p>GPT-RAG includes a User Feedback Loop feature that lets users evaluate assistant responses through the UI. Feedback is sent to the backend, processed by the orchestrator, and stored in Cosmos DB for analysis and continuous improvement.</p> <p> User feedback stored in Cosmos DB</p> <p>By default, basic feedback (thumbs up/down) is enabled, while detailed ratings (star rating and comments) are disabled. Administrators control these options through Azure App Configuration.</p>"},{"location":"userfeedback/#feedback-types","title":"Feedback Types","text":"<p>When enabled, users can provide star ratings and text comments for richer feedback that captures both satisfaction and reasoning.</p> <p> User providing rating and comment feedback</p>"},{"location":"userfeedback/#configuration-settings","title":"Configuration Settings","text":"<p>The behavior of the feedback loop is controlled by key-values in Azure App Configuration:</p> <ul> <li>ENABLE_USER_FEEDBACK \u2192 Default: <code>true</code>   Controls whether the feedback feature is available at all.</li> </ul> <p> Key to enable or disable user feedback globally</p> <ul> <li>USER_FEEDBACK_RATING \u2192 Default: <code>false</code>   Controls whether users can provide detailed feedback with ratings and comments.</li> </ul> <p> Key to enable or disable detailed rating feedback</p>"},{"location":"userfeedback/#default-values","title":"Default Values","text":"<ul> <li><code>ENABLE_USER_FEEDBACK = true</code></li> <li><code>USER_FEEDBACK_RATING = false</code></li> </ul> <p>This means feedback is collected by default, but star ratings and comments must be explicitly enabled by setting <code>USER_FEEDBACK_RATING</code> to <code>true</code>.</p>"},{"location":"whatisnew/","title":"What's New","text":"<p>\ud83d\udccc Check out what's coming next  (Azure org only)</p>"},{"location":"whatisnew/#october-2025","title":"October 2025","text":"<p>Release 2.2.0 - Agentic Retrieval and Network Flexibility</p> <p>This release introduces major enhancements to support more flexible and enterprise-ready deployments.</p> <p>Bring Your Own VNet   Enables organizations to deploy GPT-RAG within their existing virtual network, maintaining full control over network boundaries, DNS, and routing policies.   #370</p> <p>Agentic Retrieval   Adds intelligent, agent-driven retrieval orchestration that dynamically selects and combines information sources for more grounded and context-aware responses.   #359</p>"},{"location":"whatisnew/#september-2025","title":"September 2025","text":"<p>Release 2.1.0 - User Feedback Loop</p> <p>Introduces a mechanism for end-users to provide thumbs-up or thumbs-down feedback on assistant responses, storing these signals alongside conversation history to continuously improve response quality.</p> <ul> <li>How to configure it: User Feedback Configuration</li> <li>Demo video:</li> </ul>"}]}