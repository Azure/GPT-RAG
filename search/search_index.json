{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#gpt-rag-solution-accelerator","title":"GPT-RAG Solution Accelerator","text":"<p>GPT-RAG is an enterprise-grade accelerator for building robust Retrieval-Augmented Generation (RAG) solutions on Azure. It provides a modular and secure foundation that integrates Azure OpenAI, AI Search, Microsoft Foundry, and modern orchestration patterns to power intelligent assistants and data-driven applications across the enterprise.</p> <p>Designed with Zero-Trust security and Infrastructure as Code (IaC) principles from the ground up, GPT-RAG accelerates production deployments while ensuring consistency, governance, and operational excellence. It supports text, image, and voice scenarios, enabling organizations to rapidly create rich multimodal experiences.</p> <p>Latest Stable Release v2.2.6  Preview Release v2.3.0  </p> <p> Zero-Trust Architecture</p>"},{"location":"#core-services","title":"Core Services","text":"Services Description Orchestrator Manages multi-agent workflows and retrieves context using Semantic Kernel and Azure AI. Web UI User interface for chat interactions, supports streaming and custom themes. Data Ingestion Extracts, chunks, and indexes enterprise data for optimized retrieval. MCP Server Implements the Model Context Protocol for tool hosting and business logic integration."},{"location":"#contributing","title":"Contributing","text":"<p>We welcome contributions from the community! Check our Contribution Guidelines for CLA, code of conduct, and PR guidelines.</p>"},{"location":"architecture/","title":"Overview","text":""},{"location":"architecture/#architecture","title":"\ud83c\udfdb\ufe0f Architecture","text":"<p>Download Visio Diagram</p>"},{"location":"architecture/#key-capabilities","title":"Key Capabilities","text":"<ul> <li> <p>Enterprise-Grade Security   Zero-Trust architecture with private endpoints, Azure Key Vault integration, and comprehensive monitoring.</p> </li> <li> <p>Flexible &amp; Customizable   Modular design with customizable orchestration, multiple interface options, and bring-your-own-resources support.</p> </li> <li> <p>Multimodal Experience   Native support for text, images, and voice with SharePoint and Fabric connectors for seamless data integration.</p> </li> <li> <p>Production Ready   Enterprise-ready infrastructure with support for CI/CD pipelines and quality evaluation integration.</p> </li> </ul>"},{"location":"contributing/","title":"Contributing","text":"<p>We appreciate contributions and suggestions for this project!</p> <p>While anyone can submit pull requests at any time, we created a contributor form to help match volunteers with areas where we need the most help. It's optional but helps us better organize and direct contributions!</p>"},{"location":"contributing/#ways-to-contribute","title":"Ways to Contribute","text":"<ul> <li>Issues: Report bugs, propose enhancements, or share feature requests.</li> <li>Comments: Engage in discussions, help others, and review proposals.</li> <li>Documentation: Improve guides and clarity for new users.</li> <li>Design: Contribute to open design discussions and new patterns.</li> <li>Tests: Strengthen reliability through unit and integration tests.</li> <li>Code: Submit fixes, enhancements, or new modules via pull requests.</li> </ul>"},{"location":"contributing/#contribution-guidelines","title":"Contribution Guidelines","text":"<p>To maintain project quality, the following items will be considered during the PR review.</p> <p>Adhering to these best practices will streamline the review process.</p> <ul> <li> <p>Target the <code>develop</code> Branch: Always direct your pull request to the <code>develop</code> branch to ensure that changes are properly integrated into the project's development workflow.</p> </li> <li> <p>Keep Pull Requests Small: Aim to make your pull requests as focused and concise as possible. This makes it easier to review and ensures quicker integration into the codebase.</p> </li> <li> <p>Associate with Prioritized Issues: Ensure that each pull request is linked to a specific, prioritized issue in the project backlog. This helps maintain alignment with project goals and ensures that work is being done on tasks of the highest importance.</p> </li> <li> <p>Include Documentation: Every new feature or functionality must be accompanied by clear documentation explaining its purpose and configuration. This ensures others can use it independently in a self-service manner.</p> </li> <li> <p>Bugs and Documentation Corrections: Pull requests that address bugs or correct documentation do not need to be associated with prioritized issues. These can be submitted directly to maintain the quality and accuracy of the project.</p> </li> <li> <p>Multi-Repo Dependencies: If your pull request has dependencies on updates in other repositories, make sure to mention this in the pull request description. Additionally, create a corresponding pull request in the other repository to ensure synchronized updates across all related projects.</p> </li> </ul>"},{"location":"contributing/#documentation","title":"Documentation","text":"<p>All project documentation is centralized in MkDocs and hosted at https://aka.ms/gpt-rag-docs. When contributing to documentation:</p> <ul> <li> <p>Use AI Tools Wisely: GitHub Copilot and similar tools can help generate documentation, but always review and refine the output. Avoid excessive use of emojis, dashes, bullets, and images. Keep documentation clean, clear, and professional.</p> </li> <li> <p>Focus on Clarity: Prioritize straightforward language and well-structured content. Documentation should be easy to read.</p> </li> <li> <p>Follow Existing Patterns: Review existing documentation pages to maintain consistency in style, formatting, and tone.</p> </li> <li> <p>Test Your Changes: Preview your documentation locally using MkDocs before submitting to ensure proper rendering.</p> </li> </ul>"},{"location":"contributing/#code-update-workflow","title":"Code Update Workflow","text":"<p>We use a simplified version of the Fork and Branch Workflow alongside Git Flow for branching strategy. The <code>main</code> branch always contains deployment-ready code, while the <code>develop</code> branch serves as our integration branch.</p> <p>Contributors create feature branches from <code>develop</code> in their forks. Once changes are completed, they submit a pull request to the <code>develop</code> branch in the upstream repository. After review and approval, reviewers merge the changes into <code>develop</code>. Weekly, maintainers group these changes into a pull request from <code>develop</code> to <code>main</code> for final review and merging.</p>"},{"location":"contributing/#process-overview","title":"Process Overview","text":"<p>This section outlines the contribution process, highlighting the key actions for both contributors and maintainers. The accompanying diagram visually represents the workflow.</p> <p></p> <p>1) Fork the Repository</p> <p>Create a copy of the GPT-RAG upstream repository under your own GitHub account.</p> <p>2) Clone Locally</p> <p>Download your forked repository to your local machine.</p> <p>3) Add Upstream </p> <p>Link the original GPT-RAG upstream repository as <code>upstream</code> to keep your fork synchronized.</p> <p>4) Create a Feature Branch</p> <p>From your fork\u2019s <code>develop</code> branch, create a feature branch for your change (e.g., <code>feature/feature_x</code>).</p> <p>5) Commit and Push Changes</p> <p>Implement your updates locally, commit, and push them to your fork on GitHub.</p> <p>6) Open and Merge the Pull Request to <code>develop</code></p> <p>Open a PR from your feature branch in your fork to the upstream repository\u2019s <code>develop</code> branch.</p> <p>7) Sync with Upstream <code>develop</code></p> <p>After your PR is merged, update your fork's <code>develop</code> branch with the latest changes from the upstream.</p> <p>8) Create a Release Branch (Maintainers)</p> <p>When the <code>develop</code> branch is ready for release, create a branch named <code>release/x.y.z</code> from your fork's <code>develop</code>. This branch will be tested and validated before merging to <code>main</code>.</p> <p>9) Open a Pull Request to Upstream <code>main</code> (Maintainers)</p> <p>Once the release is validated, open a PR from your release branch to the upstream <code>main</code>. After the merge, maintainers will create a version tag (e.g., <code>v2.0.1</code>).</p> <p>10) Sync Your Fork</p> <p>Finally, update both your fork\u2019s <code>main</code> and <code>develop</code> branches to reflect the latest upstream state.</p>"},{"location":"contributing/#step-by-step","title":"Step-by-Step","text":"<p>Here\u2019s an example of implementing a feature called <code>conversation-metadata</code> in the <code>gpt-rag-orchestrator</code> repository.</p> <p>1) Create a Fork</p> <pre><code>https://github.com/&lt;your-github-user&gt;/gpt-rag-orchestrator.git\n</code></pre> <p>2) Clone Your Fork Locally</p> <pre><code>git clone https://github.com/&lt;your-github-user&gt;/gpt-rag-orchestrator.git\n</code></pre> <p>3) Set Upstream Remote</p> <pre><code>git remote add upstream git@github.com:Azure/gpt-rag-orchestrator.git\n</code></pre> <p>4) Create a Feature Branch</p> <pre><code>git checkout -b feature/conversation-metadata develop\n</code></pre> <p>5) Make and Push Your Changes</p> <pre><code>git add .\ngit commit -m \"Implemented conversation metadata\"\ngit push origin feature/conversation-metadata\n</code></pre> <p>6) Open and Merge the Pull Request to <code>develop</code></p> <ul> <li>6a. Create the PR:      Go to your fork on GitHub \u2192 click New Pull Request \u2192      Base: <code>Azure/gpt-rag-orchestrator</code> \u2192 <code>develop</code>      Compare: <code>&lt;your-github-user&gt;/gpt-rag-orchestrator</code> \u2192 <code>feature/conversation-metadata</code></li> <li>6b. Maintainer Review:      The maintainers will review, request changes if needed, and merge the PR into the upstream <code>develop</code>.</li> </ul> <p>7) Sync Your Fork\u2019s <code>develop</code></p> <pre><code>git fetch upstream\ngit checkout develop\ngit merge upstream/develop\ngit push origin develop\n</code></pre> <p>8) Create a Release Branch (Maintainers)</p> <pre><code>git checkout -b release/2.0.1 develop\ngit push origin release/2.0.1\n</code></pre> <p>9) Open a Pull Request to Upstream <code>main</code> (Maintainers)</p> <ul> <li>Base: <code>Azure/gpt-rag-orchestrator</code> \u2192 <code>main</code></li> <li>Compare: <code>&lt;your-github-user&gt;/gpt-rag-orchestrator</code> \u2192 <code>release/2.0.1</code></li> <li>After review and merge, maintainers tag the release (e.g., <code>v2.0.1</code>).</li> </ul> <p>10) Sync Your Fork After Tag Creation</p> <pre><code>git fetch upstream\ngit checkout main\ngit merge upstream/main\ngit push origin main\n</code></pre> <p>Note for Documentation Contributions: When contributing to documentation, target the <code>docs</code> branch instead of <code>develop</code>. It is recommended to create a dedicated clone specifically for documentation work to avoid mixing documentation changes with code updates. This keeps documentation workflows separate and simplifies the review process.</p>"},{"location":"contributing/#legal-and-code-of-conduct","title":"Legal and Code of Conduct","text":"<p>Before contributing, you'll need to sign a Contributor License Agreement (CLA) to confirm that you have the rights to, and do, grant us permission to use your contribution. More details can be found at Microsoft CLA.</p> <p>This project adheres to the Microsoft Open Source Code of Conduct. For more information, please visit the Code of Conduct FAQ or contact opencode@microsoft.com with any questions or comments.</p>"},{"location":"deploy/","title":"\ud83d\ude80 Deployment Guide","text":"<p>Choose your preferred deployment method based on project requirements and environment constraints.</p> <p>Note: You can change parameter values in <code>main.parameters.json</code> or set them with <code>azd env set</code> before running <code>azd provision</code>. This applies only to parameters that support environment variable substitution.</p>"},{"location":"deploy/#prerequisites","title":"Prerequisites","text":"<p>Required Permissions:</p> <ul> <li>Azure subscription with Contributor and User Access Admin roles</li> <li>Agreement to Responsible AI terms for Azure AI Services</li> </ul> <p>Required Tools:</p> <ul> <li>Azure Developer CLI</li> <li>PowerShell 7+ (Windows only)</li> <li>Git</li> <li>Python 3.12</li> </ul>"},{"location":"deploy/#basic-deployment","title":"Basic Deployment","text":"<p>Quick setup for demos without network isolation.</p> <pre><code>azd init -t azure/gpt-rag\naz login\nazd auth login\nazd provision\n</code></pre> <p>Add <code>--tenant</code> for <code>az</code> or <code>--tenant-id</code> for <code>azd</code> if you want a specific tenant.</p> <p>Demo video:</p>"},{"location":"deploy/#zero-trust-deployment","title":"Zero Trust Deployment","text":"<p>For deployments that require network isolation.</p> <p>Before Provisioning</p> <p>Enable network isolation in your environment:</p> <pre><code>azd env set NETWORK_ISOLATION true\n</code></pre> <p>Make sure you\u2019re signed in with your Azure user account:</p> <pre><code>az login\nazd auth login\n</code></pre> <p>Add <code>--tenant</code> for <code>az</code> or <code>--tenant-id</code> for <code>azd</code> if you want a specific tenant.</p> <p>Provision Infrastructure</p> <pre><code>azd provision\n</code></pre> <p>Post-Provision Configuration</p> <p>After provisioning completes, you'll be prompted whether your machine has VNet access.</p> <ul> <li>If you have VNet access:</li> </ul> <p>Answer <code>Y</code> when prompted. The post-provision script will run automatically and complete the configuration.</p> <ul> <li>If you don't have VNet access:</li> </ul> <p>Answer <code>N</code> when prompted. You'll need to use the provisioned Jumpbox VM to complete the post-provision steps.</p> <p>Using the Jumpbox VM</p> <p>1) Reset the VM password in the Azure Portal (required on first access if not set in deployment parameters):</p> <ul> <li>Go to your VM resource \u2192 Support + troubleshooting \u2192 Reset password \u2192 Set new credentials</li> <li>Default username is <code>testvmuser</code></li> </ul> <p>2) Connect via Azure Bastion</p> <p>3) Authenticate with the VM's Managed Identity:</p> <pre><code>az login --identity\nazd auth login --managed-identity\n</code></pre> <p>Add <code>--tenant</code> for <code>az</code> or <code>--tenant-id</code> for <code>azd</code> if you want a specific tenant.</p> <p>4) Run the post-provision script:</p> <p>PowerShell:    <pre><code>cd c:\\github\\gpt-rag\n.\\scripts\\postProvision.ps1\n</code></pre></p> <p>Bash:    <pre><code>cd /mnt/c/github/gpt-rag\n./scripts/postProvision.sh\n</code></pre></p> <p>Note: If you have re-initialized or cloned the gpt-rag repo again, refresh your <code>azd</code> environment before running the postProvision script so it points to the existing deployment: <code>azd init -t azure/gpt-rag</code> then <code>azd env refresh</code>. When prompted, select the same Subscription, Resource Group, and Location as the original provisioning so <code>azd</code> correctly links to your environment.</p>"},{"location":"deploy/#deploy-gpt-rag-services","title":"Deploy GPT-RAG Services","text":"<p>Note: For Zero Trust deployments with network isolation, ensure you have VNet connectivity either through the Jumpbox VM or via VPN before deploying services. If using the Jumpbox VM, the repositories are located in the <code>c:\\github</code> directory.</p> <p>Once the GPT-RAG infrastructure is provisioned, you can deploy the services.</p> <p>To deploy all services at once, navigate to the <code>gpt-rag</code> directory (with azd environment configured) and run:</p> <pre><code>azd deploy\n</code></pre> <p>This command deploys each service in sequence.</p> <p>If you prefer to deploy a single service, for example, when updating only that service, you can deploy it individually. Below is an example using the orchestrator service. The same approach applies to other services (frontend, dataingest, mcp).</p>"},{"location":"deploy/#deploy-individual-services","title":"Deploy Individual Services","text":"<p>Make sure you're logged in to Azure:</p> <pre><code>az login\n</code></pre> <p>Example: Deploying the Orchestrator</p> <p>Using azd (recommended):</p> <p>Initialize the template: <pre><code>azd init -t azure/gpt-rag-orchestrator \n</code></pre></p> <p>Important: Use the same environment name with <code>azd init</code> as in the infrastructure deployment to keep components consistent.</p> <p>Update environment variables then deploy: <pre><code>azd env refresh\nazd deploy \n</code></pre></p> <p>Important: Run <code>azd env refresh</code> with the same subscription and resource group used in the infrastructure deployment.</p> <p>Using a shell script:</p> <p>Clone the repository, set the App Configuration endpoint, and run the deployment script.</p> <p>PowerShell (Windows): <pre><code>git clone https://github.com/Azure/gpt-rag-orchestrator.git\n$env:APP_CONFIG_ENDPOINT = \"https://&lt;your-app-config-name&gt;.azconfig.io\"\ncd gpt-rag-orchestrator\n.\\scripts\\deploy.ps1\n</code></pre></p> <p>Bash (Linux/macOS): <pre><code>git clone https://github.com/Azure/gpt-rag-orchestrator.git\nexport APP_CONFIG_ENDPOINT=\"https://&lt;your-app-config-name&gt;.azconfig.io\"\ncd gpt-rag-orchestrator\n./scripts/deploy.sh\n</code></pre></p>"},{"location":"deploy/#permissions","title":"Permissions","text":"<p>Microsoft Foundry Role and AI Search Assignments</p> Resource Role Assignee Description GenAI App Search Service Search Index Data Reader Microsoft Foundry Project Read index data GenAI App Search Service Search Service Contributor Microsoft Foundry Project Create AI Search connection GenAI App Storage Account Storage Blob Data Reader Microsoft Foundry Project Read blob data Microsoft Foundry Account Cognitive Services User Search Service Allow Search Service to access vectorizers <p>Container App Role Assignments</p> Resource Role Assignee Description GenAI App Configuration Store App Configuration Data Reader ContainerApp: orchestrator Read configuration data GenAI App Configuration Store App Configuration Data Reader ContainerApp: frontend Read configuration data GenAI App Configuration Store App Configuration Data Reader ContainerApp: dataingest Read configuration data GenAI App Configuration Store App Configuration Data Reader ContainerApp: mcp Read configuration data GenAI App Container Registry AcrPull ContainerApp: orchestrator Pull container images GenAI App Container Registry AcrPull ContainerApp: frontend Pull container images GenAI App Container Registry AcrPull ContainerApp: dataingest Pull container images GenAI App Container Registry AcrPull ContainerApp: mcp Pull container images GenAI App Key Vault Key Vault Secrets User ContainerApp: orchestrator Read secrets GenAI App Key Vault Key Vault Secrets User ContainerApp: frontend Read secrets GenAI App Key Vault Key Vault Secrets User ContainerApp: dataingest Read secrets GenAI App Key Vault Key Vault Secrets User ContainerApp: mcp Read secrets GenAI App Search Service Search Index Data Reader ContainerApp: orchestrator Read index data GenAI App Search Service Search Index Data Contributor ContainerApp: dataingest Read/write index data GenAI App Search Service Search Index Data Contributor ContainerApp: mcp Read/write index data GenAI App Storage Account Storage Blob Data Reader ContainerApp: orchestrator Read blob data GenAI App Storage Account Storage Blob Data Reader ContainerApp: frontend Read blob data GenAI App Storage Account Storage Blob Data Contributor ContainerApp: dataingest Read/write blob data GenAI App Storage Account Storage Blob Data Contributor ContainerApp: mcp Read/write blob data GenAI App Cosmos DB Cosmos DB Built-in Data Contributor ContainerApp: orchestrator Read/write Cosmos DB data Microsoft Foundry Account Cognitive Services User ContainerApp: orchestrator Access Cognitive Services Microsoft Foundry Account Cognitive Services User ContainerApp: dataingest Access Cognitive Services Microsoft Foundry Account Cognitive Services User ContainerApp: mcp Access Cognitive Services Microsoft Foundry Account Cognitive Services OpenAI User ContainerApp: orchestrator Use OpenAI APIs Microsoft Foundry Account Cognitive Services OpenAI User ContainerApp: dataingest Use OpenAI APIs Microsoft Foundry Account Cognitive Services OpenAI User ContainerApp: mcp Use OpenAI APIs <p>Executor Role Assignments</p> Resource Role Assignee Description GenAI App Configuration Store App Configuration Data Owner Executor Full control over configuration settings GenAI App Container Registry AcrPush Executor Push container images GenAI App Container Registry AcrPull Executor Pull container images GenAI App Key Vault Key Vault Contributor Executor Manage Key Vault settings GenAI App Key Vault Key Vault Secrets Officer Executor Create Key Vault secrets GenAI App Search Service Search Service Contributor Executor Create/update search service elements GenAI App Search Service Search Index Data Contributor Executor Read/write search index data GenAI App Search Service Search Index Data Reader Executor Read index data GenAI App Storage Account Storage Blob Data Contributor Executor Read/write blob data GenAI App Cosmos DB Cosmos DB Built-in Data Contributor Executor Read/write Cosmos DB data Microsoft Foundry Account Cognitive Services OpenAI User Executor Use OpenAI APIs <p>Jumpbox VM Role Assignments</p> Resource Role Assignee Description GenAI App Container Apps Container Apps Contributor Jumpbox VM Full control over Container Apps Azure Managed Identity Managed Identity Operator Jumpbox VM Assign and manage user-assigned identities GenAI App Container Registry Container Registry Repository Writer Jumpbox VM Write to ACR repositories GenAI App Container Registry Container Registry Tasks Contributor Jumpbox VM Manage ACR tasks GenAI App Container Registry Container Registry Data Access Configuration Administrator Jumpbox VM Manage ACR data access configuration GenAI App Container Registry AcrPush Jumpbox VM Push container images GenAI App Configuration Store App Configuration Data Owner Jumpbox VM Full control over configuration settings GenAI App Key Vault Key Vault Contributor Jumpbox VM Manage Key Vault settings GenAI App Key Vault Key Vault Secrets Officer Jumpbox VM Create Key Vault secrets GenAI App Search Service Search Service Contributor Jumpbox VM Create/update search service elements GenAI App Search Service Search Index Data Contributor Jumpbox VM Read/write search index data GenAI App Storage Account Storage Blob Data Contributor Jumpbox VM Read/write blob data GenAI App Cosmos DB Cosmos DB Built-in Data Contributor Jumpbox VM Read/write Cosmos DB data Microsoft Foundry Account Cognitive Services Contributor Jumpbox VM Manage Cognitive Services resources Microsoft Foundry Account Cognitive Services OpenAI User Jumpbox VM Use OpenAI APIs"},{"location":"howto_azure_direct/","title":"Azure Direct Models","text":"<p>Azure Direct Models are models hosted and exposed by Azure AI Foundry that you call directly via the Foundry inference APIs, using Microsoft Entra ID\u2013based authentication, instead of Azure OpenAI\u2013specific APIs. This page explains how to switch GPT-RAG\u2019s default inference model (by default, <code>gpt-4o</code>) to a different model. </p> <p>This enables you to:</p> <ul> <li>Use non\u2013Azure OpenAI models (for example, Mistral, DeepSeek, Grok, etc.) from Azure.</li> <li>Standardize on Foundry inference APIs with Entra ID identity-based access.</li> </ul> <p></p> <p>How to find which models you can use</p> <p>In practice, model availability depends on what\u2019s enabled/available for your tenant/subscription/region and what appears in the Azure AI Foundry model catalog.</p> <p>To discover Azure Direct models, open the Model catalog in Azure AI Foundry and select the collection: Direct from Azure.</p> <p>Use the image below as reference (catalog + collection filter):</p> <p></p> <p></p> <p>How to switch GPT-RAG\u2019s default model</p> <p>GPT-RAG provisions with a default model (currently, <code>gpt-4o</code>). You can switch the inference model in two common ways:</p> <ul> <li>Option A (recommended): choose the model before provisioning (IaC-driven) by changing <code>infra/main.parameters.json</code>.</li> <li>Option B: choose the model after provisioning by creating/selecting the model manually in Azure AI Foundry and then updating your deployed configuration to point to it.</li> </ul> <p>Option A: Before provisioning (IaC / <code>main.parameters.json</code>)</p> <ul> <li>Reference file: https://github.com/Azure/GPT-RAG/blob/main/infra/main.parameters.json</li> </ul> <p>In <code>main.parameters.json</code>, update the <code>model</code> section.</p> <p>Example (default)</p> <pre><code>\"model\": {\n  \"format\": \"OpenAI\",\n  \"name\": \"gpt-4o\",\n  \"version\": \"2024-11-20\"\n}\n</code></pre> <p>Example (Grok)</p> <p>Change <code>format</code>, <code>name</code>, and <code>version</code> to match the model you selected in the catalog.</p> <pre><code>\"model\": {\n  \"format\": \"xAI\",\n  \"name\": \"grok-4-fast-non-reasoning\",\n  \"version\": \"1\"\n}\n</code></pre> <p>Models tested</p> <p>I tested the end-to-end flow with these models:</p> <ul> <li><code>DeepSeek-V3.1</code></li> <li><code>DeepSeek-V3-0324</code></li> <li><code>mistral-small-2503</code></li> <li><code>grok-4-fast-non-reasoning</code></li> <li><code>grok-4</code></li> </ul> <p>Note: available names/versions can change over time. Treat the Azure AI Foundry model catalog (collection Direct from Azure) as the source of truth.</p> <p>Option B: After provisioning (manual model creation)</p> <p>If you already provisioned GPT-RAG (or you prefer managing models manually), you can still switch models. The key idea is:</p> <ol> <li>Create/select the model deployment in Azure AI Foundry.</li> <li>Update GPT-RAG\u2019s configuration so the orchestrator starts calling the new model.</li> </ol> <p>1) Create/select the model in Azure AI Foundry</p> <ul> <li>In the Azure AI Foundry Model catalog, pick a model from Direct from Azure.</li> <li>Create/select a deployment (or whatever the UI calls it for that model).</li> <li>Note the model identifiers you need to configure GPT-RAG with: <code>format</code>, <code>name</code>, and <code>version</code>.</li> </ul> <p>2) Point GPT-RAG to the new model</p> <p>After provisioning, this usually means updating the runtime configuration used by the running services (rather than changing IaC inputs).</p> <ul> <li>If your deployment stores configuration in Azure App Configuration, update the corresponding model settings there and restart/redeploy the services so they pick up the new values.</li> </ul> <p>Tip: if you want a reproducible, \u201cinfrastructure-as-code\u201d change that is applied during provisioning, use Option A.</p> <p></p> <p>Bing Grounding Citations (behavior by model type) When the Bing Grounding Tool is used in Azure AI Foundry, the model may emit citation placeholders in the response text, for example: \u30100:0\u2020source\u3011. How these placeholders are resolved depends on the model:</p> <p>OpenAI / Azure OpenAI models (e.g., GPT-4, GPT-4o):</p> <p>The response text includes annotations containing a <code>url_citation</code> object with the source URL and title. The orchestrator processes these annotations and replaces placeholders with Markdown links in the format <code>[title](url)</code>.</p> <p>Other models (Azure Direct) (e.g., <code>grok-4</code>, Llama, etc.):</p> <p>The response text contains placeholders only, without annotations. Since there is no reliable way to infer the source URLs, the placeholders are removed to avoid strange characters in the final answer. Result: Bing citations appear without links when using these models.</p>"},{"location":"howto_sharepoint_connector/","title":"SharePoint Setup (Preview Release)","text":"<p>This section explains how to configure SharePoint as a data source for the GPT-RAG Azure AI Search Index, using the <code>Sites.Selected</code> permission to limit access to specific site collections.</p> <p>The configuration process involves the following steps:</p> <ol> <li>Exploring SharePoint with Graph API: Navigate and discover SharePoint site information using Microsoft Graph API.</li> <li>App Registration: Create an Azure Entra ID application to enable the solution to connect to SharePoint via Graph API.</li> <li>Ingestion Job: Configure the data sources to define which SharePoint sites, lists, and drives will be indexed.</li> <li>Run Job: Enable and schedule the ingestion jobs using CRON expressions in App Configuration.</li> <li>Validation: Test and verify that the SharePoint content is being successfully indexed.</li> </ol> <p>Using <code>Sites.Selected</code> ensures that your application only has access to the SharePoint sites you've explicitly granted permissions to, enhancing security by limiting access scope</p>"},{"location":"howto_sharepoint_connector/#exploring-sharepoint-with-graph-api","title":"Exploring SharePoint with Graph API","text":"<p>Before configuring the ingestion job, you need to understand your SharePoint site structure. This section shows you how to use Microsoft Graph API to discover site information, lists, columns, items, and permissions.</p> <p>Prerequisites</p> <ul> <li>Access to Microsoft Graph Explorer</li> <li>A user account with access to the SharePoint site you want to explore</li> <li>Basic understanding of your SharePoint site URL structure</li> </ul> <p>Understanding SharePoint Site URLs</p> <p>A typical SharePoint site URL follows this pattern:</p> <pre><code>https://{siteDomain}/sites/{siteName}\n</code></pre> <p>For example: <code>https://contoso.sharepoint.com/sites/SalesAndMarketing</code></p> <ul> <li>siteDomain: <code>contoso.sharepoint.com</code></li> <li>siteName: <code>SalesAndMarketing</code></li> </ul> <p>1. Get Site Information</p> <p>Retrieve basic information about your SharePoint site, including the site ID which you'll need for subsequent queries.</p> <p>Endpoint (Generic)</p> <pre><code>GET https://graph.microsoft.com/v1.0/sites/{siteDomain}:/sites/{siteName}\n</code></pre> <p>Example</p> <pre><code>GET https://graph.microsoft.com/v1.0/sites/m365x03100047.sharepoint.com:/sites/SalesAndMarketing\n</code></pre> <p>Response</p> <pre><code>{\n  \"id\": \"m365x03100047.sharepoint.com,9f1d115d-9c87-4f37-ba8e-e73ad39c3405,7a3700a7-3a8a-4942-9089-ad84ce9f5837\",\n  \"displayName\": \"Sales and Marketing\",\n  \"name\": \"SalesAndMarketing\",\n  \"webUrl\": \"https://m365x03100047.sharepoint.com/sites/SalesAndMarketing\"\n}\n</code></pre> <p>\ud83d\udca1 Save the <code>id</code> value - this is your <code>{siteId}</code> for all subsequent queries.</p> <p>2. Get Site Lists</p> <p>Retrieve all lists and document libraries from the site. This includes both standard SharePoint lists and document libraries.</p> <p>Endpoint (Generic)</p> <pre><code>GET https://graph.microsoft.com/v1.0/sites/{siteId}/lists\n</code></pre> <p>Example</p> <pre><code>GET https://graph.microsoft.com/v1.0/sites/m365x03100047.sharepoint.com,9f1d115d-9c87-4f37-ba8e-e73ad39c3405,7a3700a7-3a8a-4942-9089-ad84ce9f5837/lists\n</code></pre> <p>Response</p> <pre><code>{\n  \"value\": [\n    {\n      \"id\": \"7691f86f-8bdc-40eb-bcf5-7a23d0e9006c\",\n      \"displayName\": \"Product List\",\n      \"name\": \"Product List\",\n      \"list\": {\n        \"template\": \"genericList\"\n      }\n    },\n    {\n      \"id\": \"1be0da74-2b71-45e0-a9d3-1ffafa7d0ba7\",\n      \"displayName\": \"Shared Documents\",\n      \"name\": \"Shared Documents\",\n      \"list\": {\n        \"template\": \"documentLibrary\"\n      }\n    }\n  ]\n}\n</code></pre> <p>\ud83d\udca1 List Type Information: The <code>list.template</code> property indicates the type of list: - <code>\"genericList\"</code>: Standard SharePoint list - <code>\"documentLibrary\"</code>: Document library for file storage</p> <p>Save the <code>id</code> value as your <code>{listId}</code> for the list you want to ingest.</p> <p>3. Get List Columns</p> <p>Retrieve all columns (fields) from a specific list. This information is essential for configuring which fields to include in your ingestion job.</p> <p>Endpoint (Generic)</p> <pre><code>GET https://graph.microsoft.com/v1.0/sites/{siteId}/lists/{listId}/columns\n</code></pre> <p>Example</p> <pre><code>GET https://graph.microsoft.com/v1.0/sites/m365x03100047.sharepoint.com,9f1d115d-9c87-4f37-ba8e-e73ad39c3405,7a3700a7-3a8a-4942-9089-ad84ce9f5837/lists/7691f86f-8bdc-40eb-bcf5-7a23d0e9006c/columns\n</code></pre> <p>Response</p> <pre><code>{\n  \"value\": [\n    {\n      \"id\": \"...\",\n      \"name\": \"Title\",\n      \"displayName\": \"Title\",\n      \"columnGroup\": \"Custom Columns\"\n    },\n    {\n      \"id\": \"...\",\n      \"name\": \"Code_x0020_Name\",\n      \"displayName\": \"Code Name\",\n      \"columnGroup\": \"Custom Columns\"\n    },\n    {\n      \"id\": \"...\",\n      \"name\": \"Product_x0020_Line\",\n      \"displayName\": \"Product Line\",\n      \"columnGroup\": \"Custom Columns\"\n    }\n  ]\n}\n</code></pre> <p>\ud83d\udca1 Column Names: Use the <code>name</code> property (not <code>displayName</code>) when configuring the <code>includeFields</code> array in your data source configuration. Note that spaces in column names are encoded as <code>_x0020_</code>.</p> <p>4. Get List Items</p> <p>Query the actual items within a list. Use the <code>expand=fields</code> parameter to see the field values.</p> <p>Endpoint (Generic)</p> <pre><code>GET https://graph.microsoft.com/v1.0/sites/{siteId}/lists/{listId}/items?expand=fields\n</code></pre> <p>Example</p> <pre><code>GET https://graph.microsoft.com/v1.0/sites/m365x03100047.sharepoint.com,9f1d115d-9c87-4f37-ba8e-e73ad39c3405,7a3700a7-3a8a-4942-9089-ad84ce9f5837/lists/7691f86f-8bdc-40eb-bcf5-7a23d0e9006c/items?expand=fields\n</code></pre> <p>Response</p> <pre><code>{\n  \"value\": [\n    {\n      \"id\": \"1\",\n      \"fields\": {\n        \"Title\": \"Product A\",\n        \"Code_x0020_Name\": \"PROD-A\",\n        \"Product_x0020_Line\": \"Electronics\",\n        \"Modified\": \"2024-01-15T10:30:00Z\"\n      }\n    },\n    {\n      \"id\": \"2\",\n      \"fields\": {\n        \"Title\": \"Product B\",\n        \"Code_x0020_Name\": \"PROD-B\",\n        \"Product_x0020_Line\": \"Home Goods\",\n        \"Modified\": \"2024-01-16T14:20:00Z\"\n      }\n    }\n  ]\n}\n</code></pre> <p>\ud83d\udca1 Testing Your Configuration: Use this endpoint to verify that your filter expressions and field selections will return the expected data before configuring the ingestion job.</p> <p>5. Get Item Permissions</p> <p>View who has access to specific list items. This helps you understand the permission structure before ingestion.</p> <p>Endpoint (Generic)</p> <p>\u26a0\ufe0f Note: The permissions endpoint requires the beta version of the Graph API (<code>/beta/</code> instead of <code>/v1.0/</code>), as permissions are not fully available in v1.0.</p> <pre><code>GET https://graph.microsoft.com/beta/sites/{siteId}/lists/{listId}/items/{itemId}/permissions\n</code></pre> <p>Example</p> <pre><code>GET https://graph.microsoft.com/beta/sites/m365x03100047.sharepoint.com,9f1d115d-9c87-4f37-ba8e-e73ad39c3405,7a3700a7-3a8a-4942-9089-ad84ce9f5837/lists/7691f86f-8bdc-40eb-bcf5-7a23d0e9006c/items/3/permissions\n</code></pre> <p>Response</p> <pre><code>{\n  \"value\": [\n    {\n      \"id\": \"...\",\n      \"roles\": [\"read\"],\n      \"grantedToIdentities\": [\n        {\n          \"user\": {\n            \"displayName\": \"John Doe\",\n            \"email\": \"john.doe@contoso.com\"\n          }\n        }\n      ]\n    }\n  ]\n}\n</code></pre> <p>\ud83d\udca1 Permission Insights: This information helps you understand who can access the content you're planning to ingest. Ensure that the ingestion app registration has appropriate permissions to access this content.</p> <p>Quick Reference: Generic Endpoints</p> <p>Copy these generic endpoints to use in Graph Explorer:</p> <pre><code># 1. Get Site Info\nGET https://graph.microsoft.com/v1.0/sites/{siteDomain}:/sites/{siteName}\n\n# 2. Get Site Lists\nGET https://graph.microsoft.com/v1.0/sites/{siteId}/lists\n\n# 3. Get List Columns\nGET https://graph.microsoft.com/v1.0/sites/{siteId}/lists/{listId}/columns\n\n# 4. Get List Items\nGET https://graph.microsoft.com/v1.0/sites/{siteId}/lists/{listId}/items?expand=fields\n\n# 5. Get Item Permissions (beta API)\nGET https://graph.microsoft.com/beta/sites/{siteId}/lists/{listId}/items/{itemId}/permissions\n</code></pre> <p>What to Collect</p> <p>As you explore your SharePoint site, collect the following information for your ingestion configuration:</p> Information Graph API Property Used In Configuration As Site ID <code>id</code> from <code>/sites/{siteDomain}:/sites/{siteName}</code> <code>siteId</code> Site Domain Part of your SharePoint URL <code>siteDomain</code> Site Name Part of your SharePoint URL <code>siteName</code> List ID <code>id</code> from <code>/sites/{siteId}/lists</code> <code>listId</code> List Name <code>name</code> from <code>/sites/{siteId}/lists</code> <code>listName</code> List Type <code>list.template</code> from <code>/sites/{siteId}/lists</code> <code>listType</code> Column Names <code>name</code> from <code>/sites/{siteId}/lists/{listId}/columns</code> <code>includeFields</code> array <p>Now that you understand how to explore your SharePoint structure, proceed to the App Registration section to create the necessary Azure Entra ID application.</p>"},{"location":"howto_sharepoint_connector/#app-registration","title":"App Registration","text":"<p>Prerequisites</p> <p>Before you begin, ensure you have the appropriate roles for each step:</p> Steps Required Role(s) Register the app and assign <code>Sites.Selected</code>. Global Administrator, Application Administrator, or Cloud Application Administrator. Grant admin consent for <code>Sites.Selected</code>. Global Administrator or Application Administrator. Retrieve the SharePoint site ID via Microsoft Graph. SharePoint Administrator, Global Administrator, or a user with access to the site. Assign site permissions via Microsoft Graph. SharePoint Administrator or Global Administrator. <p>Procedure</p> <ol> <li> <p>Register an Application in Azure Entra ID</p> <ul> <li>Sign in to the Azure Portal: Go to Azure Portal.</li> <li>Register a New Application:<ul> <li>Navigate to Azure Active Directory &gt; App registrations &gt; New registration.</li> <li>Name: Enter a name for your application (e.g., <code>SharePointDataIngestionApp</code>).</li> <li>Supported Account Types: Choose Accounts in this organizational directory only.</li> <li>Redirect URI: Leave this field empty.</li> <li>Click Register.</li> </ul> </li> </ul> <p></p> <ul> <li>Record Application IDs:<ul> <li>Save the Application ID and Tenant ID for later use.</li> </ul> </li> </ul> <p></p> </li> <li> <p>Configure API Permissions</p> <ul> <li> <p>Navigate to API Permissions:</p> <ul> <li>In your registered application, go to API permissions &gt; Add a permission.</li> </ul> </li> <li> <p>Add Microsoft Graph Permissions:</p> <ul> <li>Select Microsoft Graph &gt; Application permissions.</li> <li>Search for and add the following permission:</li> <li><code>Sites.Selected</code></li> </ul> </li> <li> <p>Click Add permissions.</p> </li> <li> <p>Grant Admin Consent:</p> <ul> <li>Click Grant admin consent for [Your Tenant Name].</li> <li>Confirm the action when prompted.</li> </ul> </li> </ul> <p> Granting admin consent for <code>Sites.Selected</code> permission</p> </li> <li> <p>Assign Access to Specific Site Collections</p> <p>The <code>Sites.Selected</code> permission requires you to explicitly grant the application access to specific site collections. This step must be performed using the Microsoft Graph API.</p> <p>Currently, assigning site permissions using <code>Sites.Selected</code> cannot be done through the Azure Portal. You need to use Microsoft Graph API or PowerShell.</p> <ul> <li> <p>Gather Site Information:</p> </li> <li> <p>Site URL: Navigate to the SharePoint site you wish to index and note its URL (e.g., <code>https://yourdomain.sharepoint.com/sites/YourSiteName</code>).</p> </li> <li>Site ID: You can retrieve the Site ID using Microsoft Graph API.</li> </ul> <p> Getting site URL</p> <ul> <li> <p>Retrieve Site ID:</p> <ul> <li> <p>Use Microsoft Graph Explorer:</p> </li> <li> <p>Go to Microsoft Graph Explorer.</p> </li> <li>Sign in with an account that has access to the site.</li> <li> <p>Make a <code>GET</code> request to:</p> <pre><code>GET https://graph.microsoft.com/v1.0/sites/{hostname}:/{server-relative-path}\n</code></pre> <p>Replace <code>{hostname}</code> with your SharePoint domain (e.g., <code>yourdomain.sharepoint.com</code>) and <code>{server-relative-path}</code> with the site path (e.g., <code>/sites/YourSiteName</code>).</p> </li> <li> <p>Example:</p> <pre><code>GET https://graph.microsoft.com/v1.0/sites/yourdomain.sharepoint.com:/sites/YourSiteName\n</code></pre> </li> <li> <p>The response will include the <code>id</code> of the site.</p> </li> </ul> </li> </ul> <p> Getting site ID </p> <ul> <li> <p>Grant the Application Access to the Site:</p> <ul> <li> <p>Make a <code>POST</code> Request to Grant Permissions:</p> </li> <li> <p>In Microsoft Graph Explorer, make a <code>POST</code> request to:</p> <pre><code>POST https://graph.microsoft.com/v1.0/sites/{site-id}/permissions\n</code></pre> <p>Replace <code>{site-id}</code> with the ID obtained in the previous step.</p> </li> <li> <p>Request Body:</p> <pre><code>{\n  \"roles\": [\"read\"],\n  \"grantedToIdentities\": [\n    {\n      \"application\": {\n        \"id\": \"your_application_id\",\n        \"displayName\": \"Your Application Name\"\n      }\n    }\n  ]\n}\n</code></pre> <ul> <li>Replace <code>your_application_id</code> with your application's Client ID.</li> <li>Replace <code>Your Application Name</code> with your application's name.</li> <li>The <code>\"roles\"</code> can be <code>\"read\"</code> or <code>\"write\"</code> depending on your needs.</li> </ul> </li> <li> <p>Example:</p> <pre><code>{\n  \"roles\": [\"read\"],\n  \"grantedToIdentities\": [\n    {\n      \"application\": {\n        \"id\": \"12345678-90ab-cdef-1234-567890abcdef\",\n        \"displayName\": \"SharePointDataIngestionApp\"\n      }\n    }\n  ]\n}\n</code></pre> </li> </ul> </li> <li> <p>Run the Query and ensure you receive a <code>201 Created</code> response.</p> </li> <li> <p>Repeat the permission assignment for each site you wish to index.</p> </li> </ul> <p> Assigning site permissions via Microsoft Graph Explorer</p> <ul> <li>If you encounter a permission denied error when trying to assign site permissions:</li> </ul> <p>If you encounter a permission error, like the one shown in the next screen, it may be necessary to grant permissions to your user.</p> <p> Permission error when assigning permissions</p> <p>If this is the case, grant the required permissions as shown in the next image.</p> <p> Adding consent for user to apply permissions</p> </li> <li> <p>Create a Client Secret</p> <ul> <li> <p>Navigate to Certificates &amp; Secrets:</p> <ul> <li>Under the Manage section of your application, select Certificates &amp; secrets.</li> </ul> </li> <li> <p>Add a New Client Secret:</p> <ul> <li>Under Client secrets, click on New client secret.</li> <li>Description: Provide a description for the client secret (e.g., <code>SharePointClientSecret</code>).</li> <li>Expires: Choose an appropriate expiration period that suits your needs.</li> <li>Click Add.</li> </ul> </li> <li> <p>Record the Client Secret Value:</p> <ul> <li>Copy and securely store the Client Secret Value for later use.</li> </ul> <p>Note: Do not copy the \"Secret ID\" as it is not required.</p> </li> </ul> <p></p> <p>Done! You have completed the necessary permissions for SharePoint. Now, to complete the configuration in your Function App:</p> </li> <li> <p>Gather SharePoint Site Information</p> <ul> <li>Site Domain: The domain of your SharePoint site (e.g., <code>yourdomain.sharepoint.com</code>).</li> <li>Site Name: The name of your SharePoint site (e.g., <code>YourSiteName</code>).</li> <li>Site Folder: Folder path to index (e.g., <code>/Shared Documents/General</code>). Leave empty for root.</li> <li>File Formats: Specify the file formats to index (e.g., <code>pdf,docx,pptx</code>).</li> </ul> </li> <li> <p>Update App Settings</p> <ul> <li> <p>Navigate to App Configuration:</p> <ul> <li>In the Azure Portal, go to your App Configuration &gt; Configuration Explorer.</li> </ul> </li> <li> <p>Set the Following App Settings with <code>gpt-rag-ingestion</code> label:</p> </li> </ul> <pre><code>SHAREPOINT_TENANT_ID=your_actual_tenant_id\nSHAREPOINT_CLIENT_ID=your_actual_client_id\nSHAREPOINT_CLIENT_SECRET_NAME=sharepoint_keyvault_secret_name (Default to sharepointClientSecret)\nSHAREPOINT_FILES_FORMAT=\"pdf,docx\"\n</code></pre> <ul> <li> <p>Replace placeholders with the actual values obtained from previous steps.</p> </li> <li> <p>Add SharePoint Client Secret to KeyVault:</p> <ul> <li>Add the SharePoint client secret value to the GPT-RAG Key Vault. You can use sharepointClientSecret as the secret name, or if you choose a custom name, make sure to add it to the <code>SHAREPOINT_CLIENT_SECRET_NAME</code> environment variable.</li> </ul> </li> </ul> <p>Leave <code>SHAREPOINT_FILES_FORMAT</code> empty to include the following default extensions: vtt, xlsx, xls, pdf, png, jpeg, jpg, bmp, tiff, docx, pptx.</p> <ul> <li> <p>Save and Restart:</p> <ul> <li>Click Save to apply the changes.</li> </ul> <p>Done! You have completed the SharePoint configuration procedure.</p> </li> </ul> </li> </ol> <p>Additional Information:</p> <ul> <li> <p>Removing Permissions:</p> <p>If you need to revoke the application's access to a site, you can delete the permission via Microsoft Graph API:</p> <pre><code>DELETE https://graph.microsoft.com/v1.0/sites/{site-id}/permissions/{permission-id}\n</code></pre> <ul> <li>You can obtain the <code>permission-id</code> by listing the permissions:</li> </ul> <pre><code>GET https://graph.microsoft.com/v1.0/sites/{site-id}/permissions\n</code></pre> </li> <li> <p>Understanding <code>Sites.Selected</code> Permission:</p> <ul> <li>The <code>Sites.Selected</code> permission by itself does not grant access to any SharePoint site collections.</li> <li>It allows your application to access only the site collections that you explicitly grant it access to.</li> <li>This approach adheres to the principle of least privilege, enhancing security.</li> </ul> </li> </ul> <p>Here is the fully regenerated section, now phrased to clearly indicate that this is a job configuration section. You can replace your entire Ingestion Job section with the content below.</p>"},{"location":"howto_sharepoint_connector/#ingestion-job","title":"Ingestion Job","text":"<p>This section explains how to configure the SharePoint ingestion jobs that run inside the Container App.</p> <p>Procedure</p> <p>1 Add Cosmos DB Data Source Configuration</p> <p>Each SharePoint site to be indexed must be stored as a document in the <code>datasources</code> container within Cosmos DB.</p> <p>Only the lists explicitly included in this configuration will be indexed. If none are defined, no SharePoint content from the site will be processed.</p> <p>\u26a0\ufe0f Before finalizing this configuration, you must retrieve required values such as <code>siteId</code>, <code>siteName</code>, <code>listId</code>, <code>listName</code>, and <code>listType</code> from Microsoft Graph API. Step 2 provides a complete step-by-step guide to obtain this information. Once collected, return to this step and complete the configuration document.</p> <p>\ud83d\udca1 Important: In this context, a list may represent either:</p> <ul> <li>a standard SharePoint list (e.g., \u201cProduct List\u201d), or</li> <li>a document library (e.g., \u201cShared Documents\u201d).   Both are returned by the <code>/lists</code> endpoint and can be indexed by the ingestion job.   When a list is a document library, the ingestion job uses the list item\u2019s <code>driveItem</code> relationship to download the underlying file.</li> </ul> <p>Data Source JSON Schema</p> <pre><code>{\n  \"id\": \"datasource-id\",\n  \"siteId\": \"unique-site-identifier\",\n  \"siteDomain\": \"yourcompany.sharepoint.com\",\n  \"siteName\": \"SiteName\",\n  \"description\": \"Human-readable description\",\n  \"type\": \"sharepoint_site\",\n  \"category\": \"Default category for all items (optional)\",\n\n  \"lists\": [\n    {\n      \"listId\": \"list-unique-identifier\",             // GUID obtained from Graph API\n      \"listName\": \"ListName\",                         // classic list or document library\n      \"listType\": \"genericList\",                      // \"genericList\" or \"documentLibrary\"\n      \"category\": \"Optional category override\",\n      \"filter\": \"OData filter expression (optional)\", // e.g., \"fields/Status eq 'Published'\"\n      \"includeFields\": [\"Title\", \"Description\"]       // optional, if empty include all fields, if documentLibrary do not include fields.\n    }\n  ]\n}\n</code></pre> <p>Behavior Rules</p> Rule Description <code>includeFields</code> empty All fields are included Always include <code>Modified</code> Required for incremental updates Include <code>Attachments</code> if needed Used for classic list attachments (not the main file in a document library) Lists not declared Not indexed <p>\ud83d\udd0e For document libraries, each file is represented as a list item with a <code>driveItem</code> relationship. The ingestion job resolves the corresponding file for each item via <code>driveItem</code> and downloads its content for indexing.</p> <p>2 Retrieve SharePoint Site and Lists (Microsoft Graph)</p> <p>Use these steps to gather the required values for the Cosmos DB configuration.</p> <p>2.1 Sign in and verify permissions</p> <p>The user performing this lookup must have delegated Microsoft Graph permissions to access the site</p> <p>2.2 Get the <code>siteId</code> and confirm the <code>siteName</code></p> <p>You will need both the <code>siteId</code> and <code>siteName</code> to complete your data source configuration.</p> <p>The <code>siteDomain</code> and <code>siteName</code> are segments of the SharePoint site URL. Example:</p> <pre><code>https://contoso.sharepoint.com/sites/SalesAndMarketing\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n            siteDomain                    siteName\n</code></pre> Parameter Example value siteDomain <code>contoso.sharepoint.com</code> siteName <code>SalesAndMarketing</code> <p>Request</p> <pre><code>GET https://graph.microsoft.com/v1.0/sites/{siteDomain}:/sites/{siteName}\n</code></pre> <p>Example</p> <pre><code>GET https://graph.microsoft.com/v1.0/sites/contoso.sharepoint.com:/sites/SalesAndMarketing\n</code></pre> <p>Expected response</p> <pre><code>{\n  \"id\": \"contoso.sharepoint.com,2b4e3b7f-1a23-4d56-89ab-123456789abc,abcdef12-3456-7890-abcd-ef0123456789\",\n  \"name\": \"SalesAndMarketing\",\n  \"displayName\": \"Sales and Marketing\"\n}\n</code></pre> <p>Use the values as follows:</p> JSON field Data Source field <code>id</code> <code>siteId</code> <code>name</code> <code>siteName</code> <p>Example mapping:</p> <pre><code>\"siteId\": \"contoso.sharepoint.com,2b4e3b7f-1a23-4d56-89ab-123456789abc,abcdef12-3456-7890-abcd-ef0123456789\",\n\"siteName\": \"SalesAndMarketing\",\n\"siteDomain\": \"contoso.sharepoint.com\"\n</code></pre> <p>2.3 List SharePoint Lists and obtain <code>listName</code></p> <pre><code>GET https://graph.microsoft.com/v1.0/sites/{siteId}/lists\n</code></pre> <p>This endpoint returns:</p> <ul> <li>Standard lists (e.g., \u201cProduct List\u201d), and</li> <li>Document libraries (e.g., \u201cShared Documents\u201d), because document libraries are implemented as lists.</li> </ul> <p>You can distinguish them with the <code>list.template</code> property:</p> <pre><code>\"list\": {\n  \"template\": \"genericList\"      // standard list\n}\n</code></pre> <pre><code>\"list\": {\n  \"template\": \"documentLibrary\"  // document library\n}\n</code></pre> <p>Example result:</p> <pre><code>{\n  \"id\": \"c3d8f2c9-1234-4567-89ab-1c2d3e4f5a6b\",\n  \"name\": \"Policies\",\n  \"displayName\": \"Policies\",\n  \"list\": {\n    \"template\": \"genericList\"\n  }\n}\n</code></pre> <p>Use <code>name</code> as <code>listName</code> in the Cosmos DB configuration.</p> <p>Optional \u2014 view fields for a specific list:</p> <pre><code>GET https://graph.microsoft.com/v1.0/sites/{siteId}/lists/{listId}/columns\n</code></pre> <p>2.4 Access document files from document libraries</p> <p>When a list is a document library (<code>template = \"documentLibrary\"</code>), each file appears as a list item that has a <code>driveItem</code> relationship.</p> <p>To retrieve list items and see their associated file:</p> <pre><code>GET https://graph.microsoft.com/v1.0/sites/{siteId}/lists/{listId}/items?expand=fields,driveItem\n</code></pre> <p>For file items, the response will include:</p> <pre><code>{\n  \"id\": \"3\",\n  \"fields\": {\n    \"Title\": \"Policy 2025\",\n    \"FileLeafRef\": \"Policy-2025.pdf\"\n  },\n  \"driveItem\": {\n    \"id\": \"01ABCDEF1234567890\",\n    \"name\": \"Policy-2025.pdf\",\n    \"file\": {\n      \"mimeType\": \"application/pdf\"\n    }\n  }\n}\n</code></pre> <p>To download the actual document content for indexing, the ingestion job can call:</p> <pre><code>GET https://graph.microsoft.com/v1.0/sites/{siteId}/lists/{listId}/items/{itemId}/driveItem/content\n</code></pre> <p>This returns the binary content of the file (PDF, DOCX, etc.), which can then be processed and indexed.</p> <p>\u2705 The ingestion job does not need a separate <code>drives</code> configuration. It resolves files for document libraries directly from list items via the <code>driveItem</code> relationship.</p> <p>Example Cosmos DB Data Source Entry</p> <pre><code>{\n  \"id\": \"contoso-marketing-site\",\n  \"siteId\": \"m365x03100047.sharepoint.com,9f1d115d-9c87-4f37-ba8e-e73ad39c3405,7a3700a7-3a8a-4942-9089-ad84ce9f5837\",\n  \"description\": \"Contoso Marketing Site\",\n  \"type\": \"sharepoint_site\",\n  \"category\": \"Marketing\",\n  \"siteDomain\": \"m365x03100047.sharepoint.com\",\n  \"siteName\": \"SalesAndMarketing\",\n\n  \"lists\": [\n    {\n      \"listId\": \"1be0da74-2b71-45e0-a9d3-1ffafa7d0ba7\",\n      \"listName\": \"Shared Documents\",\n      \"listType\": \"documentLibrary\",\n      \"category\": \"Contoso Documents\",\n      \"filter\": \"\",\n    },\n    {\n      \"listId\": \"7691f86f-8bdc-40eb-bcf5-7a23d0e9006c\",\n      \"listName\": \"Product List\",\n      \"listType\": \"genericList\",\n      \"category\": \"Contoso Products\",\n      \"filter\": \"\",\n      \"includeFields\": [\"Title\", \"Code_x0020_Name\", \"Product_x0020_Line\", \"Notes\", \"Country\"]\n    }\n  ]\n}\n</code></pre>"},{"location":"howto_sharepoint_connector/#run-ingestion-job","title":"Run Ingestion Job","text":"<p>The SharePoint ingestion process is executed by scheduled jobs running inside the Container App. These jobs are responsible for:</p> <ul> <li>Indexing new or updated SharePoint content</li> <li>Purging deleted or removed content from the Azure AI Search index</li> </ul> <p>Execution frequency is controlled by CRON expressions defined in App Configuration Settings under the label <code>gpt-rag-ingestion</code>.</p> <ol> <li> <p>Update App Settings</p> <ul> <li> <p>Navigate to App Configuration:</p> <ul> <li>In the Azure Portal, go to your App Configuration &gt; Configuration Explorer.</li> </ul> </li> <li> <p>Set the Following App Settings with <code>gpt-rag-ingestion</code> label:</p> </li> </ul> <pre><code>SHAREPOINT_CONNECTOR_ENABLED=true\nCRON_RUN_SHAREPOINT_INDEX=0 0 * * * *\nCRON_RUN_SHAREPOINT_PURGE=0 0 */6 * * *\n</code></pre> Setting Description <code>CRON_RUN_SHAREPOINT_INDEX</code> Defines how often the indexing job runs <code>CRON_RUN_SHAREPOINT_PURGE</code> Defines how often the purge job runs <p>CRON Format: <code>{second} {minute} {hour} {day} {month} {day-of-week}</code></p> </li> </ol>"},{"location":"howto_sharepoint_connector/#validation","title":"Validation","text":"<ol> <li> <p>Test Data Ingestion</p> <ul> <li> <p>Trigger the Ingestion Process:</p> <ul> <li>Wait for the data ingestion scheduled run.</li> </ul> </li> <li> <p>Monitor Logs:</p> <ul> <li>Check the Function App logs to verify that the SharePoint connector is running without errors.</li> </ul> </li> </ul> </li> <li> <p>Verify Indexed Data</p> <ul> <li> <p>Check Azure AI Index:</p> <ul> <li>Go to your Azure AI Index to confirm that the SharePoint data has been successfully indexed.</li> </ul> </li> <li> <p>Perform Search Queries:</p> <ul> <li>Execute search queries to ensure that content from the specific SharePoint sites is retrievable.</li> </ul> </li> </ul> </li> </ol>"},{"location":"howto_userfeedback/","title":"User Feedback Configuration","text":"<p>GPT-RAG includes a User Feedback Loop feature that lets users evaluate assistant responses through the UI. Feedback is sent to the backend, processed by the orchestrator, and stored in Cosmos DB for analysis and continuous improvement.</p> <p> User feedback stored in Cosmos DB</p> <p>By default, basic feedback (thumbs up/down) is enabled, while detailed ratings (star rating and comments) are disabled. Administrators control these options through Azure App Configuration.</p>"},{"location":"howto_userfeedback/#feedback-types","title":"Feedback Types","text":"<p>When enabled, users can provide star ratings and text comments for richer feedback that captures both satisfaction and reasoning.</p> <p> User providing rating and comment feedback</p>"},{"location":"howto_userfeedback/#configuration-settings","title":"Configuration Settings","text":"<p>The behavior of the feedback loop is controlled by key-values in Azure App Configuration:</p> <ul> <li>ENABLE_USER_FEEDBACK \u2192 Default: <code>true</code>   Controls whether the feedback feature is available at all.</li> </ul> <p> Key to enable or disable user feedback globally</p> <ul> <li>USER_FEEDBACK_RATING \u2192 Default: <code>false</code>   Controls whether users can provide detailed feedback with ratings and comments.</li> </ul> <p> Key to enable or disable detailed rating feedback</p>"},{"location":"howto_userfeedback/#default-values","title":"Default Values","text":"<ul> <li><code>ENABLE_USER_FEEDBACK = true</code></li> <li><code>USER_FEEDBACK_RATING = false</code></li> </ul> <p>This means feedback is collected by default, but star ratings and comments must be explicitly enabled by setting <code>USER_FEEDBACK_RATING</code> to <code>true</code>.</p>"},{"location":"ingestion_blob_data_source/","title":"Blob Data Source","text":"<p>The Blob Data Source ingests documents from the <code>documents</code> container in your Azure Storage Account into Azure AI Search and keeps the index synchronized when files are updated or removed. It is designed for production-scale document processing with incremental updates, smart freshness detection, and automated cleanup.</p>"},{"location":"ingestion_blob_data_source/#how-it-works","title":"How it Works","text":"<p>The Blob Data Source operates through two independent jobs that can be scheduled separately for optimal resource usage and data freshness.</p> <p>Indexing job (<code>blob-storage-indexer</code>) scans the configured blob container (optionally filtered by <code>BLOB_PREFIX</code>) and processes documents into searchable chunks. It uses smart freshness detection by comparing each blob's <code>last_modified</code> timestamp with the indexed version, skipping unchanged files to save processing time and costs. For each new or modified file, the indexer downloads the blob, chunks it using Azure Document Intelligence, deletes any existing chunks for that file (by <code>parent_id</code>), and uploads new chunks with stable, search-safe IDs. Each chunk document sets <code>source = \"blob\"</code> to enable cleanup operations, and when available in blob metadata, includes <code>metadata_security_id</code> for security trimming scenarios.</p> <p>Purging job (<code>blob-storage-purger</code>) maintains index hygiene by removing orphaned documents. It compares parent documents in blob storage with parent documents in the AI Search index (where <code>source == \"blob\"</code>), identifies chunks belonging to files that no longer exist in storage, and deletes them in batches. The job waits briefly after deletion to ensure accurate document counts, as Azure AI Search uses eventual consistency for index statistics.</p> <p>Files are processed in parallel using a configurable semaphore (<code>INDEXER_MAX_CONCURRENCY</code>, default: 4) to balance throughput with service limits. Each file goes through download \u2192 security metadata extraction \u2192 document chunking \u2192 chunk conversion \u2192 batch upload. Batch uploads use <code>INDEXER_BATCH_SIZE</code> (default: 500) to optimize AI Search API calls while staying within request size limits.</p> <p>Supported formats include PDF, Word (.docx), PowerPoint (.pptx), Excel (.xlsx), text (.txt, .md), images (.jpg, .png, .bmp, .tiff), and HTML. OCR extraction is performed automatically for PDFs and images using Azure Document Intelligence.</p>"},{"location":"ingestion_blob_data_source/#ingestion-flow","title":"Ingestion Flow","text":"<p>Indexing Job</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                      BLOB STORAGE INGESTION FLOW                               \u2502\n\u2502                                                                                \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502  Azure Blob        \u2502           \u2502      Index State Cache                  \u2502  \u2502\n\u2502  \u2502  Storage           \u2502           \u2502  \u2022 Load existing timestamps             \u2502  \u2502\n\u2502  \u2502  \u2022 Source Container\u2502\u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502  \u2022 Build parent_id \u2192 lastModified map   \u2502  \u2502\n\u2502  \u2502  \u2022 Blob Metadata   \u2502           \u2502  \u2022 Detect already-indexed docs          \u2502  \u2502\n\u2502  \u2502  \u2022 Security IDs    \u2502           \u2502    (incremental updates)                \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502            \u2502                                                                   \u2502\n\u2502            \u2502 List Blobs + Filter by Prefix                                     \u2502\n\u2502            v                                                                   \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502                      Freshness Check                                    \u2502   \u2502\n\u2502  \u2502                                                                         \u2502   \u2502\n\u2502  \u2502  For each blob:                                                         \u2502   \u2502\n\u2502  \u2502    \u2022 Compare blob.last_modified vs indexed timestamp                    \u2502   \u2502\n\u2502  \u2502    \u2022 Skip if blob.last_modified \u2264 indexed timestamp                     \u2502   \u2502\n\u2502  \u2502    \u2022 Queue for processing if blob is newer or not indexed               \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502                                  \u2502                                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                   \u2502\n                                   v\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                          PROCESSING PIPELINE                                   \u2502\n\u2502                                                                                \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502                    blob_storage_indexer.py                               \u2502  \u2502\n\u2502  \u2502                                                                          \u2502  \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502  \u2502\n\u2502  \u2502  \u2502  Download    \u2502  \u2502  Security    \u2502  \u2502  Document    \u2502  \u2502  Chunk       \u2502  \u2502  \u2502\n\u2502  \u2502  \u2502  Blob        \u2502\u2500&gt;\u2502  Metadata    \u2502\u2500&gt;\u2502  Chunker     \u2502\u2500&gt;\u2502  Conversion  \u2502  \u2502  \u2502\n\u2502  \u2502  \u2502  (Binary)    \u2502  \u2502  Extraction  \u2502  \u2502  (PDF/Docs)  \u2502  \u2502  to Search   \u2502  \u2502  \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502  \u2502\n\u2502  \u2502                                                               \u2502          \u2502  \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                           \u2502          \u2502  \u2502\n\u2502  \u2502  \u2502  Index       \u2502  \u2502  Delete Old  \u2502\u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2502  \u2502\n\u2502  \u2502  \u2502  Batch       \u2502\u25c4\u2500\u2502  Parent Docs \u2502  (Remove existing chunks)            \u2502  \u2502\n\u2502  \u2502  \u2502  Upload      \u2502  \u2502  by parent_id\u2502                                      \u2502  \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                      \u2502  \u2502\n\u2502  \u2502                                                                          \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502                                                                                \u2502\n\u2502  \u2022 Parallel Processing: Configurable semaphore (INDEXER_MAX_CONCURRENCY)       \u2502\n\u2502  \u2022 Batch Size: 500 docs per AI Search batch (INDEXER_BATCH_SIZE)               \u2502\n\u2502  \u2022 Error Handling: Per-blob try/catch with individual file logs                \u2502\n\u2502                                                                                \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                   \u2502\n                                   v\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                            OUTPUT &amp; STORAGE                                    \u2502\n\u2502                                                                                \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502  Azure AI Search     \u2502  \u2502  Azure Blob Storage  \u2502  \u2502    Telemetry         \u2502  \u2502\n\u2502  \u2502  \u2022 Indexed Chunks    \u2502  \u2502  \u2022 Run Summaries     \u2502  \u2502  \u2022 App Insights      \u2502  \u2502\n\u2502  \u2502  \u2022 Vector Embeddings \u2502  \u2502  \u2022 Per-File Logs     \u2502  \u2502  \u2022 Structured Logs   \u2502  \u2502\n\u2502  \u2502  \u2022 Security IDs      \u2502  \u2502  \u2022 Processing State  \u2502  \u2502  \u2022 Performance       \u2502  \u2502\n\u2502  \u2502  \u2022 Metadata          \u2502  \u2502  (jobs container)    \u2502  \u2502  \u2022 Error Tracking    \u2502  \u2502\n\u2502  \u2502  source=blob         \u2502  \u2502                      \u2502  \u2502                      \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502                                                                                \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"ingestion_blob_data_source/#scheduling","title":"Scheduling","text":"<p>Jobs are enabled through CRON expressions:</p> <ul> <li><code>CRON_RUN_BLOB_INDEX</code>: runs the indexing job</li> <li><code>CRON_RUN_BLOB_PURGE</code>: runs the purge job</li> <li>Leave unset to disable</li> </ul> <p>The scheduler uses <code>SCHEDULER_TIMEZONE</code> (IANA format, e.g., <code>Europe/Berlin</code>), falling back to the host machine\u2019s timezone if not specified. On startup, if a CRON is configured, the corresponding job is scheduled and also triggered once immediately.</p> <p>Examples:</p> <ul> <li><code>0 * * * *</code> \u2192 hourly</li> <li><code>*/15 * * * *</code> \u2192 every 15 minutes</li> <li><code>0 0 * * *</code> \u2192 daily at midnight</li> </ul>"},{"location":"ingestion_blob_data_source/#settings","title":"Settings","text":"<ul> <li><code>STORAGE_ACCOUNT_NAME</code> and <code>DOCUMENTS_STORAGE_CONTAINER</code>: source location</li> <li><code>SEARCH_SERVICE_QUERY_ENDPOINT</code> and <code>SEARCH_RAG_INDEX_NAME</code>: target index</li> <li><code>BLOB_PREFIX</code> (optional): restricts the scan scope</li> <li><code>JOBS_LOG_CONTAINER</code> (default: jobs): container for logs</li> <li><code>INDEXER_MAX_CONCURRENCY</code>: concurrency, defaults: <code>4</code>.</li> <li><code>INDEXER_BATCH_SIZE</code>: batch size, defaults: <code>500</code></li> </ul> <p><code>INDEXER_MAX_CONCURRENCY</code> controls how many files are processed in parallel (download \u2192 chunk \u2192 upload). <code>INDEXER_BATCH_SIZE</code> controls how many chunk documents are sent in each upload call to Azure AI Search. Increase these to raise throughput, but watch for throttling (HTTP 429), timeouts, and memory usage; lower them if you see retries or instability. The default batch size (500) follows common guidance to keep batches reasonable (typically \u2264 1000).</p>"},{"location":"ingestion_blob_data_source/#logs","title":"Logs","text":"<p>Both jobs write logs to the configured jobs container. Logs are grouped by job type:</p> <ul> <li> <p>Indexer (<code>blob-storage-indexer</code>)</p> <ul> <li>Per-file logs and per-run summaries under <code>files/</code> and <code>runs/</code></li> <li>Summaries include: <code>sourceFiles</code>, <code>candidates</code>, <code>success/failed</code>, <code>totalChunksUploaded</code></li> </ul> </li> <li> <p>Purger (<code>blob-storage-purger</code>)</p> <ul> <li>Per-run summaries under <code>runs/</code></li> <li>Summaries include: <code>blobDocumentsCount</code>, <code>indexParentsCountBefore/After</code>, <code>indexChunkDocumentsBefore</code>, <code>indexParentsPurged</code>, <code>indexChunkDocumentsDeleted</code></li> </ul> </li> </ul>"},{"location":"ingestion_blob_data_source/#observability","title":"Observability","text":"<p>The blob storage indexer emits structured Application Insights events (<code>RUN-*</code>, <code>ITEM-*</code>) with JSON payloads embedded in the <code>message</code> field.</p> <p>Latest Job Runs</p> <p>View recent indexing jobs with key metrics:</p> <pre><code>let Logs = union isfuzzy=true traces, AppTraces;\nLogs\n| where message contains \"RUN-COMPLETE\" and message contains \"blob-storage-indexer\"\n| extend payload = parse_json(extract('\\\\{.*', 0, message))\n| where tostring(payload.event) == \"RUN-COMPLETE\"\n| project timestamp,\n          runId = tostring(payload.runId),\n          status = tostring(payload.status),\n          sourceFiles = toint(payload.sourceFiles),\n          itemsDiscovered = toint(payload.itemsDiscovered),\n          indexedItems = toint(payload.indexedItems),\n          skippedNoChange = toint(payload.skippedNoChange),\n          failed = toint(payload.failed),\n          totalChunksUploaded = toint(payload.totalChunksUploaded),\n          durationSeconds = todouble(payload.durationSeconds)\n| order by timestamp desc\n</code></pre> <p>Purge Summary Stats</p> <p>Inspect the most recent purge runs and their cleanup metrics:</p> <pre><code>let Logs = union isfuzzy=true traces, AppTraces;\nLogs\n| where message contains \"blob-storage-purger\"\n    and message contains \"Summary:\"\n| extend payload = parse_json(extract('\\\\{.*', 0, message))\n| extend runStartedAt  = todatetime(payload.runStartedAt),\n                 runFinishedAt = todatetime(payload.runFinishedAt)\n| project\n        timestamp,\n        runStartedAt,\n        runFinishedAt,\n        durationSeconds           = datetime_diff(\"second\", runFinishedAt, runStartedAt),\n        blobDocumentsCount        = toint(payload.blobDocumentsCount),\n        indexParentsCountBefore   = toint(payload.indexParentsCountBefore),\n        indexChunkDocumentsBefore = toint(payload.indexChunkDocumentsBefore),\n        indexParentsPurged        = toint(payload.indexParentsPurged),\n        indexChunkDocumentsDeleted= toint(payload.indexChunkDocumentsDeleted),\n        indexParentsCountAfter    = toint(payload.indexParentsCountAfter)\n| order by timestamp desc\n</code></pre> <p>Items in Specific Indexer Run</p> <p>List all files processed during a particular run:</p> <pre><code>let TargetRunId = '20251121T231125Z';\nlet Logs = union isfuzzy=true traces, AppTraces;\nLogs\n| where message contains \"ITEM-COMPLETE\" and message contains \"blob-storage-indexer\"\n| extend payload = parse_json(extract('\\\\{.*', 0, message))\n| where tostring(payload.event) == \"ITEM-COMPLETE\" and tostring(payload.runId) == TargetRunId\n| project timestamp,\n          blobName = tostring(payload.blobName),\n          parentId = tostring(payload.parentId),\n          status = tostring(payload.status),\n          totalChunks = toint(payload.totalChunks),\n          contentType = tostring(payload.contentType),\n          fileUrl = tostring(payload.fileUrl)\n| order by timestamp desc\n</code></pre> <p>File Indexing History</p> <p>Track processing history for a specific file:</p> <pre><code>let TargetParent = '/documents/employee_handbook.pdf';\nlet Logs = union isfuzzy=true traces, AppTraces;\nLogs\n| where message contains \"ITEM-COMPLETE\" and message contains \"blob-storage-indexer\"\n| extend payload = parse_json(extract('\\\\{.*', 0, message))\n| where tostring(payload.event) == \"ITEM-COMPLETE\" and tostring(payload.parentId) == TargetParent\n| project timestamp,\n          runId = tostring(payload.runId),\n          blobName = tostring(payload.blobName),\n          status = tostring(payload.status),\n          totalChunks = toint(payload.totalChunks),\n          contentType = tostring(payload.contentType),\n          fileUrl = tostring(payload.fileUrl)\n| order by timestamp desc\n</code></pre> <p>Recent Indexer Errors</p> <p>View recent warnings and errors:</p> <pre><code>let Logs = union isfuzzy=true traces, AppTraces;\nLogs\n| where severityLevel &gt;= 3 and message contains \"blob-storage-indexer\"\n| extend payload = parse_json(extract('\\\\{.*', 0, message))\n| project timestamp,\n          severityLevel,\n          event = tostring(payload.event),\n          runId = tostring(payload.runId),\n          blobName = tostring(payload.blobName),\n          parentId = tostring(payload.parentId),\n          error = tostring(payload.error),\n          message\n| order by timestamp desc\n</code></pre>"},{"location":"ingestion_nl2sql_data_source/","title":"NL2SQL / Fabric Metadata Data Source","text":"<p>The NL2SQL ingestion process loads structured metadata from blob storage\u2014including tables, measures, and sample queries\u2014into Azure AI Search. This metadata provides the orchestrator with the necessary context to generate accurate SQL and DAX queries.</p> <p>Each JSON file defines the metadata for one entity (a table, measure, or query).</p>"},{"location":"ingestion_nl2sql_data_source/#how-it-works","title":"How It Works","text":""},{"location":"ingestion_nl2sql_data_source/#push-based-indexing","title":"Push-based Indexing","text":"<p>The service reads JSON files directly from blob storage, validates their schemas, and generates embeddings with the configured model. The metadata is then inserted or updated in Azure AI Search.</p>"},{"location":"ingestion_nl2sql_data_source/#updates-change-detection","title":"Updates (Change Detection)","text":"<p>Updates follow an upsert approach: existing metadata is replaced, new metadata is added. Change detection uses blob metadata (<code>Last-Modified</code>, <code>ETag</code>), so only modified or missing entries are re-indexed, while unchanged ones are skipped.</p>"},{"location":"ingestion_nl2sql_data_source/#purging","title":"Purging","text":"<p>A purge job compares index entries with the JSON files in blob storage. Any entry without a matching file is removed, keeping the index consistent with the source.</p>"},{"location":"ingestion_nl2sql_data_source/#metadata-structure","title":"Metadata Structure","text":"<p>NL2SQL and Chat-with-Fabric rely on metadata to give the orchestrator structured information for optimized SQL and DAX queries. The metadata consists of three types:</p> <ul> <li>Table metadata: Descriptions of tables and columns</li> <li>Query metadata: Example SQL or DAX queries with natural-language intent</li> <li>Measure metadata: Definitions of Power BI semantic model measures</li> </ul>"},{"location":"ingestion_nl2sql_data_source/#folder-structure","title":"Folder Structure","text":"<p>Store metadata JSON files in dedicated folders:</p> <ul> <li><code>tables/</code> \u2014 Table metadata</li> <li><code>measures/</code> \u2014 Measure metadata</li> <li><code>queries/</code> \u2014 Query metadata</li> </ul> <p>Suggested filenames:</p> <ul> <li>Table metadata: <code>dimension_city.json</code></li> <li>Measure metadata: <code>total_revenue.json</code></li> <li>Query metadata: <code>top_5_expensive_products.json</code></li> </ul>"},{"location":"ingestion_nl2sql_data_source/#metadata-types","title":"Metadata Types","text":""},{"location":"ingestion_nl2sql_data_source/#table-metadata","title":"Table Metadata","text":"<p>Elements</p> <ul> <li><code>table</code>: table name</li> <li><code>description</code>: purpose of the table</li> <li><code>datasource</code>: datasource ID</li> <li> <p><code>columns</code>: list of columns</p> </li> <li> <p><code>name</code></p> </li> <li><code>description</code></li> <li><code>type</code> (optional)</li> <li><code>examples</code> (optional)</li> </ul> <p>Example</p> <pre><code>{\n  \"table\": \"sales_order\",\n  \"description\": \"Table containing sales order data.\",\n  \"datasource\": \"wwi-sales-data\",\n  \"columns\": [\n    { \"name\": \"OrderID\", \"description\": \"Unique identifier for each sales order.\" },\n    { \"name\": \"OrderDate\", \"description\": \"Date when the sales order was placed.\" }\n  ]\n}\n</code></pre>"},{"location":"ingestion_nl2sql_data_source/#query-metadata","title":"Query Metadata","text":"<p>Elements</p> <ul> <li><code>datasource</code>: datasource ID</li> <li><code>question</code>: natural-language question the query answers</li> <li><code>query</code>: SQL or DAX text</li> <li><code>reasoning</code> (optional)</li> </ul> <p>Example</p> <pre><code>{\n  \"datasource\": \"adventureworks\",\n  \"question\": \"What are the top 5 most expensive products currently available for sale?\",\n  \"query\": \"SELECT TOP 5 ProductID, Name, ListPrice FROM SalesLT.Product WHERE SellEndDate IS NULL ORDER BY ListPrice DESC\",\n  \"reasoning\": \"Retrieves top 5 products by ListPrice that are currently for sale.\"\n}\n</code></pre>"},{"location":"ingestion_nl2sql_data_source/#measure-metadata","title":"Measure Metadata","text":"<p>Elements</p> <ul> <li><code>datasource</code>: datasource ID</li> <li><code>name</code>: measure name</li> <li><code>description</code>: what it calculates</li> <li><code>type</code>: <code>external</code> or <code>local</code></li> <li><code>source_table</code> (local only)</li> <li><code>data_type</code>, <code>source_model</code> (external only)</li> </ul> <p>Example</p> <pre><code>{\n  \"datasource\": \"Ecommerce\",\n  \"name\": \"Total Revenue (%)\",\n  \"description\": \"Calculates percentage of total revenue for the selected period.\",\n  \"type\": \"external\",\n  \"data_type\": \"CURRENCY\",\n  \"source_model\": \"Executive Sales Dashboard\"\n}\n</code></pre> <p>More examples are available in the <code>samples/</code> folder.</p>"},{"location":"ingestion_nl2sql_data_source/#nl2sql-data-sources","title":"NL2SQL Data Sources","text":"<p>Supported datasource types:</p> <ul> <li>semantic_model \u2014 Runs DAX queries via Power BI REST API</li> <li>sql_endpoint \u2014 Connects via ODBC with a Service Principal</li> <li>sql_database \u2014 Connects via ODBC with Managed Identity</li> </ul>"},{"location":"ingestion_nl2sql_data_source/#data-sources-configuration","title":"Data Sources Configuration","text":"<p>Data sources contain connection information for the target systems.</p> <p>Supported data source types include:</p> <ul> <li>Semantic Model: Executes DAX queries using the Power BI REST API.</li> <li>SQL Endpoint: Connects via ODBC using a Service Principal.</li> <li>SQL Database: Connects via ODBC using Managed Identity.</li> </ul> <p>Data source configuration in GPT-RAG is managed through JSON documents stored in the datasources container of Cosmos DB.</p> <p> Sample Datasource Configuration</p>"},{"location":"ingestion_nl2sql_data_source/#semantic-model-example","title":"Semantic Model Example","text":"<pre><code>{\n  \"id\": \"wwi-sales-aggregated-data\",\n  \"description\": \"Aggregated sales data for insights such as sales by employee or city.\",\n  \"type\": \"semantic_model\",\n  \"organization\": \"myorg\",\n  \"dataset\": \"your_dataset_or_semantic_model_name\",\n  \"tenant_id\": \"your_sp_tenant_id\",\n  \"client_id\": \"your_sp_client_id\"\n}\n</code></pre>"},{"location":"ingestion_nl2sql_data_source/#sql-endpoint-example","title":"SQL Endpoint Example","text":"<pre><code>{\n  \"id\": \"wwi-sales-star-schema\",\n  \"description\": \"Star schema with sales data and dimension tables.\",\n  \"type\": \"sql_endpoint\",\n  \"organization\": \"myorg\",\n  \"server\": \"your_sql_endpoint.fabric.microsoft.com\",\n  \"database\": \"your_lakehouse_name\",\n  \"tenant_id\": \"your_sp_tenant_id\",\n  \"client_id\": \"your_sp_client_id\"\n}\n</code></pre>"},{"location":"ingestion_nl2sql_data_source/#sql-database-example","title":"SQL Database Example","text":"<pre><code>{\n  \"id\": \"adventureworks\",\n  \"description\": \"AdventureWorksLT database with customers, orders, and products.\",\n  \"type\": \"sql_database\",\n  \"database\": \"adventureworkslt\",\n  \"server\": \"sqlservername.database.windows.net\"\n}\n</code></pre> <p>For data sources that require secrets\u2014such as those accessed via a Service Principal or SQL Server using SQL authentication\u2014passwords are stored in Azure Key Vault following the naming convention {datasource_id}-secret.</p> <p>Example: If the datasource_id is wwi-sales-star-schema, the corresponding secret name in Key Vault should be wwi-sales-star-schema-secret.</p> <p> Sample Datasource Secrets</p> <p>Example configuration files are available in the sample folder.</p>"},{"location":"ingestion_nl2sql_data_source/#settings","title":"Settings","text":"<p>Add the following configuration values to App Configuration Settings, using either the <code>gpt-rag</code> or <code>gpt-rag-ingestion</code> labels:</p> <ul> <li><code>STORAGE_ACCOUNT_NAME</code> \u2014 The storage account where metadata JSON files are located.</li> <li><code>NL2SQL_STORAGE_CONTAINER</code> \u2014 The container holding metadata files (default: <code>nl2sql</code>).</li> <li><code>JOBS_LOG_CONTAINER</code> \u2014 The container used for logs (default: <code>jobs</code>).</li> <li><code>INDEXER_MAX_CONCURRENCY</code> \u2014 Maximum number of files processed concurrently (default: <code>4</code>).</li> <li><code>SEARCH_QUERIES_INDEX_NAME</code> \u2014 Index name for query metadata.</li> <li><code>SEARCH_TABLES_INDEX_NAME</code> \u2014 Index name for table metadata.</li> <li><code>SEARCH_MEASURES_INDEX_NAME</code> \u2014 Index name for measure metadata.</li> </ul> <p>If an index name is not provided, documents of that type will not be indexed, and a warning will be logged. The embedding model configuration is automatically taken from the GPT-RAG Microsoft Foundry settings defined during infrastructure deployment.</p>"},{"location":"ingestion_nl2sql_data_source/#scheduling","title":"Scheduling","text":"<p>Because ingestion is push-based, updates are triggered by a CRON schedule for periodic refreshes.</p> <p>Add the following settings to App Configuration Settings, using either the <code>gpt-rag</code> or <code>gpt-rag-ingestion</code> labels:</p> <ul> <li><code>CRON_RUN_NL2SQL_INDEX</code> \u2014 Runs the NL2SQL indexing job.</li> <li><code>CRON_RUN_NL2SQL_PURGE</code> \u2014 Runs the NL2SQL purge job.</li> </ul> <p>If you do not set a CRON expression for one of these values, that job will not run automatically. For example, if <code>CRON_RUN_NL2SQL_PURGE</code> is left empty or not defined, purge operations will never be scheduled.</p>"},{"location":"ingestion_nl2sql_data_source/#examples-of-cron-expressions","title":"Examples of CRON expressions","text":"<ul> <li><code>0 * * * *</code> \u2192 Runs hourly.</li> <li><code>*/15 * * * *</code> \u2192 Runs every 15 minutes.</li> <li><code>0 0 * * *</code> \u2192 Runs daily at midnight.</li> </ul>"},{"location":"ingestion_nl2sql_data_source/#timezone","title":"Timezone","text":"<p>The scheduler uses the <code>SCHEDULER_TIMEZONE</code> setting (IANA format, e.g., <code>Europe/Berlin</code>). If not specified, it defaults to the host machine\u2019s timezone.</p>"},{"location":"ingestion_nl2sql_data_source/#startup-behavior","title":"Startup behavior","text":"<p>When the application starts, if a CRON expression is configured, the corresponding job is scheduled and also triggered once immediately.</p>"},{"location":"ingestion_nl2sql_data_source/#logs","title":"Logs","text":"<p>Logs are stored in <code>JOBS_LOG_CONTAINER</code>:</p> <ul> <li> <p>Runs (<code>nl2sql-indexer/runs/</code>)</p> <ul> <li>One summary JSON per run (<code>{runId}.json</code>)</li> <li>Includes totals for query, table, and measure metadata</li> </ul> </li> <li> <p>Files (<code>nl2sql-indexer/files/</code>)</p> <ul> <li>One JSON per file, named after the blob path (slashes replaced with dashes)</li> <li>Includes blob metadata, embedding vector size, status, and errors (if any)</li> </ul> </li> </ul> <p>These logs show skipped files, re-indexed metadata, failures, and per-run statistics.</p>"},{"location":"ingestion_sharepoint_source/","title":"SharePoint Data Source","text":"<p>The SharePoint connector keeps Azure AI Search synchronized with both structured list data and rich documents stored in SharePoint Online. It is designed for production-scale ingestion jobs where resiliency, incremental freshness, and search-ready chunking matter.</p>"},{"location":"ingestion_sharepoint_source/#how-it-works","title":"How it Works","text":"<p>The SharePoint connector ingests both generic lists (structured metadata) and document libraries (files like PDFs, Word, PowerPoint) into Azure AI Search. It uses smart freshness detection by comparing SharePoint's last modified timestamp with the indexed version, skipping unchanged items to save processing time. The connector processes all collections in parallel but controls worker concurrency and OpenAI API calls to avoid rate limits.</p> <p>Generic lists are indexed by reading item fields from Microsoft Graph API. You can control which fields get embedded using the optional <code>includeFields</code> configuration. Lookup columns are automatically resolved and cached per list, except for hidden system lists like <code>AppPrincipals</code>, <code>UserInfo</code>, or taxonomy stores. List item attachments are currently not downloaded.</p> <p>Document libraries download files (default: <code>pdf</code>, <code>docx</code>, <code>pptx</code>) and chunk them using Azure Document Intelligence. Each chunk gets a zero-based ID (<code>c00000</code>, <code>c00001</code>, etc.) which enables accurate freshness checks\u2014if SharePoint's <code>lastModifiedDateTime</code> hasn't changed since the last index run, the file is skipped without reprocessing.</p> <p>Permissions are handled by calling <code>get_item_permission_object_ids</code> using Graph beta <code>/permissions</code> endpoint to capture explicit Entra user/group IDs for each item. Only GUID-backed identities (users, groups, app registrations, devices) are stored.</p> <p>For detailed setup instructions, including app registration, permissions, and data source configuration, see the SharePoint Connector Setup Guide.</p>"},{"location":"ingestion_sharepoint_source/#ingestion-flow","title":"Ingestion Flow","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                         SHAREPOINT INGESTION FLOW                              \u2502\n\u2502                                                                                \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502  Microsoft Graph   \u2502           \u2502         Cosmos DB                       \u2502  \u2502\n\u2502  \u2502  API Client        \u2502           \u2502  \u2022 Site Configurations                  \u2502  \u2502\n\u2502  \u2502  \u2022 Site Discovery  \u2502&lt;\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502  \u2022 List/Library Specs                   \u2502  \u2502\n\u2502  \u2502  \u2022 List/Library    \u2502           \u2502  \u2022 Field Mappings                       \u2502  \u2502\n\u2502  \u2502  \u2022 Permissions     \u2502           \u2502  \u2022 Category Metadata                    \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502            \u2502                                                                   \u2502\n\u2502            \u2502 Pull Items + Metadata                                             \u2502\n\u2502            v                                                                   \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502                    SharePoint Collections                               \u2502   \u2502\n\u2502  \u2502                                                                         \u2502   \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502   \u2502\n\u2502  \u2502  \u2502  Generic Lists   \u2502              \u2502  Document Libraries          \u2502     \u2502   \u2502\n\u2502  \u2502  \u2502  \u2022 Custom Fields \u2502              \u2502  \u2022 Office Files (docx/pptx)  \u2502     \u2502   \u2502\n\u2502  \u2502  \u2502  \u2022 Lookup Fields \u2502              \u2502  \u2022 PDFs                      \u2502     \u2502   \u2502\n\u2502  \u2502  \u2502  \u2022 List Items    \u2502              \u2502  \u2022 Binary Content            \u2502     \u2502   \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502   \u2502\n\u2502  \u2502            \u2502                                   \u2502                        \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502               \u2502                                   \u2502                            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u2502                                   \u2502\n                v                                   v\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                          PROCESSING PIPELINE                                   \u2502\n\u2502                                                                                \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502                    sharepoint_indexer.py                                 \u2502  \u2502\n\u2502  \u2502                                                                          \u2502  \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502  \u2502\n\u2502  \u2502  \u2502  Freshness   \u2502  \u2502  Security    \u2502  \u2502  Lookup      \u2502  \u2502  Content     \u2502  \u2502  \u2502\n\u2502  \u2502  \u2502  Check       \u2502\u2500&gt;\u2502  Permissions \u2502\u2500&gt;\u2502  Field       \u2502\u2500&gt;\u2502  Extraction  \u2502  \u2502  \u2502\n\u2502  \u2502  \u2502  (Last Mod)  \u2502  \u2502  Resolution  \u2502  \u2502  Resolution  \u2502  \u2502  + Chunking  \u2502  \u2502  \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502  \u2502\n\u2502  \u2502                                                               \u2502          \u2502  \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                           \u2502          \u2502  \u2502\n\u2502  \u2502  \u2502  Azure       \u2502  \u2502  Document    \u2502&lt;\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2502  \u2502\n\u2502  \u2502  \u2502  OpenAI      \u2502&lt;\u2500\u2502  Chunker     \u2502  (For attachments)                   \u2502  \u2502\n\u2502  \u2502  \u2502  Embeddings  \u2502  \u2502  (PDFs/Docs) \u2502                                      \u2502  \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                      \u2502  \u2502\n\u2502  \u2502         \u2502                                                                \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502            \u2502                                                                   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n             \u2502\n             v\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                            OUTPUT &amp; STORAGE                                    \u2502\n\u2502                                                                                \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502  Azure AI Search     \u2502  \u2502  Azure Blob Storage  \u2502  \u2502    Telemetry         \u2502  \u2502\n\u2502  \u2502  \u2022 Indexed Chunks    \u2502  \u2502  \u2022 Run Summaries     \u2502  \u2502  \u2022 App Insights      \u2502  \u2502\n\u2502  \u2502  \u2022 Vector Embeddings \u2502  \u2502  \u2022 Item Logs         \u2502  \u2502  \u2022 Structured Logs   \u2502  \u2502\n\u2502  \u2502  \u2022 Security IDs      \u2502  \u2502  \u2022 Processing State  \u2502  \u2502  \u2022 Performance       \u2502  \u2502\n\u2502  \u2502  \u2022 Metadata          \u2502  \u2502                      \u2502  \u2502  \u2022 Error Tracking    \u2502  \u2502\n\u2502  \u2502  source=sharepoint   \u2502  \u2502                      \u2502  \u2502                      \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502                                                                                \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"ingestion_sharepoint_source/#processing-pipeline","title":"Processing Pipeline","text":"<p>The indexer uses three tiers of parallelism to balance speed and service limits:</p> <p>1. Collection Discovery (All Lists in Parallel)</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Cosmos datasources (type: sharepoint_site)                   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n            \u2502                        \u2502                   \u2502\n            \u25bc                        \u25bc                   \u25bc\n      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510             \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n      \u2502  List A  \u2502             \u2502  List B  \u2502   ...   \u2502  List N  \u2502   \u2190 All lists start simultaneously\n      \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518             \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518\n            \u2502                        \u2502                    \u2502\n            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                             \u2502\n                             \u25bc (fetch items via Graph API)\n</code></pre> <p>2. Item Processing (Controlled: \u2264 4 Workers)</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Global Worker Pool (INDEXER_MAX_CONCURRENCY = 4)                \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                 \u2502\n\u2502  \u2502Worker 1 \u2502 \u2502Worker 2 \u2502 \u2502Worker 3 \u2502 \u2502  ...4   \u2502                 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        \u2502           \u2502           \u2502           \u2502\n        \u25bc           \u25bc           \u25bc           \u25bc\n   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502 Per-Item: freshness \u2192 process \u2192 upload                  \u2502\n   \u2502 \u2022 Body (generic lists): list fields \u2192 text \u2192 embedding  \u2502\n   \u2502 \u2022 Files (document libraries): download \u2192 chunk \u2192 embed  \u2502\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>3. Embedding Generation (Throttled: \u2264 2 Concurrent)</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  AOAI Embedding Gate (AOAI_MAX_CONCURRENCY = 2)        \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                             \u2502\n\u2502  \u2502  Slot 1  \u2502 \u2502  Slot 2  \u2502  \u2190 Only 2 embeddings run    \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     at the same time        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u25b2              \u25b2\n         \u2502              \u2502\n    Workers queue here when embedding is needed\n    (workers skip this if freshness = unchanged)\n</code></pre> <p>Parallelism At a Glance</p> Layer Control Default Notes Collection enumeration <code>asyncio.gather</code> Unlimited Each list runs independently. Item workers Semaphore <code>INDEXER_MAX_CONCURRENCY = 4</code> Covers body + file work. Embeddings Semaphore <code>AOAI_MAX_CONCURRENCY = 2</code> Applies to both bodies and files. Item timeout <code>asyncio.wait_for</code> 600\u202fs Cancels sluggish items. Collection timeout <code>asyncio.wait_for</code> 7200\u202fs Cancels stuck lists. <p>Freshness &amp; Deduplication</p> <ul> <li>Body documents (generic lists): Fetches chunk <code>c00000</code> from the index. If SharePoint's <code>Modified</code> timestamp isn't newer, the item is skipped (<code>skippedNoChange</code>) without reprocessing.</li> <li>Document library files: Each file gets a parent key with the file name; chunk <code>0</code> stores the file's last modified time. Unchanged files increment <code>documentLibraryStats.skippedNotNewer</code>.</li> <li>1-second tolerance: An item is reindexed only if SharePoint's timestamp is &gt;1 second newer than the index. This prevents unnecessary work when clocks differ slightly between SharePoint and Azure AI Search.</li> </ul>"},{"location":"ingestion_sharepoint_source/#settings-cheat-sheet","title":"Settings Cheat Sheet","text":"Category Setting Default Purpose Concurrency <code>INDEXER_MAX_CONCURRENCY</code> 4 Item workers across all lists. <code>AOAI_MAX_CONCURRENCY</code> 2 Embedding throttle. <code>INDEXER_BATCH_SIZE</code> 500 Upload/delete batch size in AI Search. Timeouts <code>INDEXER_ITEM_TIMEOUT_SECONDS</code> 600 Per-item budget. Cancels stuck workers. <code>LIST_GATHER_TIMEOUT_SECONDS</code> 7200 Per-list budget. Aborts entire list if exceeded. <code>HTTP_TOTAL_TIMEOUT_SECONDS</code> 120 Graph API calls timeout. <code>BLOB_OP_TIMEOUT_SECONDS</code> 20 Blob storage writes timeout. Retries <code>AOAI_BACKOFF_MAX_SECONDS</code> 60 Max wait between AOAI retries (exponential backoff + jitter). <code>AOAI_MAX_RATE_LIMIT_ATTEMPTS</code> 8 Rate limit (429) retries for embeddings. Respects <code>Retry-After</code> headers. <code>AOAI_MAX_TRANSIENT_ATTEMPTS</code> 8 Network/timeout retries for embeddings. Fatal errors bubble immediately. <code>GRAPH_RETRY_ATTEMPTS</code> 6 Microsoft Graph GET retries for throttling/transient failures. Max 30s backoff. <code>SEARCH_RETRY_ATTEMPTS</code> 8 Azure AI Search upload/delete retries (1s \u2192 30s backoff). Honors <code>Retry-After</code>. Documents <code>SHAREPOINT_FILES_FORMAT</code> <code>pdf,docx,pptx</code> Allowed file extensions for document libraries. Logging <code>JOBS_LOG_CONTAINER</code> <code>jobs</code> Blob container for logs. <code>DISABLE_STORAGE_LOGS</code> unset Set to <code>true/1</code> to skip blob logging. <p>Tuning notes: Increase <code>AOAI_MAX_CONCURRENCY</code> only if you confirmed higher TPM quotas. If Graph throttles (429), reduce <code>INDEXER_MAX_CONCURRENCY</code>. Document Intelligence chunker performs best-effort retries internally; failed items (e.g., 503 errors) can be retried on next run.</p>"},{"location":"ingestion_sharepoint_source/#observability","title":"Observability","text":"<p>The indexer writes logs to two destinations: Application Insights (always active) and Azure Blob Storage (optional).</p> <p>Application Insights</p> <p>All indexer activity flows to Application Insights automatically. Below are the four most requested queries:</p> <p>1. Latest indexer runs</p> <pre><code>let Logs = union isfuzzy=true traces, AppTraces;\nLogs\n| where message contains \"RUN-COMPLETE\" and message contains \"sharepoint-indexer\"\n| extend payload = parse_json(extract('\\\\{.*', 0, message))\n| where tostring(payload.event) == \"RUN-COMPLETE\"\n| extend indexerType = extract('\\\\[([^\\\\]]+)\\\\]', 1, message)\n| where indexerType endswith \"-indexer\"  // Filtra apenas indexers\n| project timestamp,\n          indexerType,\n          runId = tostring(payload.runId),\n          status = tostring(payload.status),\n          itemsDiscovered = toint(payload.itemsDiscovered),\n          itemsIndexed = toint(payload.itemsIndexed),\n          itemsFailed = toint(payload.itemsFailed),\n          durationSeconds = todouble(payload.durationSeconds)\n| order by timestamp desc\n</code></pre> <p>2. All items indexed in a specific run</p> <pre><code>let TargetRunId = '20251121T212623Z';\nlet Logs = union isfuzzy=true traces, AppTraces;\nLogs\n| where message contains \"ITEM-COMPLETE\"\n| extend payload = parse_json(extract('\\\\{.*', 0, message))\n| where tostring(payload.event) == \"ITEM-COMPLETE\" and tostring(payload.runId) == TargetRunId\n| project timestamp,\n          collection = tostring(payload.collection),\n          itemId = tostring(payload.itemId),\n          parentId = tostring(payload.parentId),\n          status = tostring(payload.status),\n          attachmentChunks = toint(payload.attachmentChunks),\n          totalChunks = toint(payload.totalChunks),\n          webUrl = tostring(payload.webUrl)\n| order by timestamp desc\n</code></pre> <p>3. Indexing history for a specific item with details</p> <pre><code>let TargetParent = '/m365x03100047.sharepoint.com/SalesAndMarketing/1be0da74-2b71-45e0-a9d3-1ffafa7d0ba7/15';\nlet Logs = union isfuzzy=true traces, AppTraces;\nLogs\n| where message contains \"ITEM-COMPLETE\"\n| extend payload = parse_json(extract('\\\\{.*', 0, message))\n| where tostring(payload.event) == \"ITEM-COMPLETE\" and tostring(payload.parentId) == TargetParent\n| project timestamp,\n          runId = tostring(payload.runId),\n          collection = tostring(payload.collection),\n          status = tostring(payload.status),\n          attachmentChunks = toint(payload.attachmentChunks),\n          totalChunks = toint(payload.totalChunks),\n          webUrl = tostring(payload.webUrl)\n| order by timestamp desc\n</code></pre> <p>4. Recent errors (all error events)</p> <pre><code>let Logs = union isfuzzy=true traces, AppTraces;\nLogs\n| where severityLevel &gt;= 3  // 3=Warning, 4=Error\n| where message contains \"sharepoint-indexer\"\n| extend payload = parse_json(extract('\\\\{.*', 0, message))\n| where isnotempty(tostring(payload.event))\n| project timestamp,\n          severityLevel,\n          event = tostring(payload.event),\n          runId = tostring(payload.runId),\n          collection = tostring(payload.collection),\n          itemId = tostring(payload.itemId),\n          parentId = tostring(payload.parentId),\n          error = tostring(payload.error),\n          message\n| order by timestamp desc\n</code></pre> <p>Blob Storage Logs (Optional)</p> <p>Blob logging is enabled by default but gracefully degrades if unavailable. To disable, set the app setting (not environment variable): <pre><code>DISABLE_STORAGE_LOGS = true\n</code></pre></p> <p>Note: Azure Functions/Container Apps use Application Settings, not shell environment variables. Set this in the Azure Portal under Configuration \u2192 Application Settings.</p> <p>When enabled, logs are written to the blob container specified by the <code>JOBS_LOG_CONTAINER</code> app setting (default: <code>jobs</code>):</p> <p>Per-Item Logs: <code>jobs/sharepoint-indexer/files/{sanitized_parent_id}.json</code></p> <p>Each processed item generates a JSON log with:</p> <ul> <li> <p>Status: <code>success</code>, <code>skipped-no-change</code>, or <code>error</code></p> </li> <li> <p>Freshness details: <code>incomingLastMod</code>, <code>existingLastMod</code>, <code>freshnessReason</code></p> </li> <li> <p>Document library metadata: <code>documentLibraryFileName</code>, <code>documentLibraryUrl</code> (if applicable)</p> </li> <li> <p>Chunks processed: Count of chunks uploaded for this item</p> </li> <li> <p>Errors: Full exception details if the item failed</p> </li> </ul> <p>Example: <pre><code>{\n  \"indexerType\": \"sharepoint-indexer\",\n  \"collection\": \"contoso.sharepoint.com/sites/engineering/Documents\",\n  \"itemId\": \"42\",\n  \"parent_id\": \"contoso_engineering_abc123_42\",\n  \"runId\": \"20251121T143022Z\",\n  \"status\": \"success\",\n  \"incomingLastMod\": \"2025-11-21T14:30:22Z\",\n  \"existingLastMod\": \"2025-11-20T10:15:00Z\",\n  \"freshnessReason\": \"newer-by-ms=102382000\",\n  \"chunks\": 3\n}\n</code></pre></p> <p>Run Summaries: <code>jobs/sharepoint-indexer/runs/{runId}.{status}.json</code> Each job execution creates stage-specific snapshots: - <code>{runId}.started.json</code>: Job initialization (collections count, start time) - <code>{runId}.finishing.json</code>: Mid-execution snapshot with partial stats - <code>{runId}.finished.json</code>: Final authoritative summary (or <code>.failed.json</code>/<code>.cancelled.json</code>) - <code>latest.json</code>: Pointer to the most recent run (best-effort; may lag on immutable containers)</p> <p>Example final summary: <pre><code>{\n  \"indexerType\": \"sharepoint-indexer\",\n  \"runId\": \"20251121T143022Z\",\n  \"runStartedAt\": \"2025-11-21T14:30:22Z\",\n  \"runFinishedAt\": \"2025-11-21T14:35:18Z\",\n  \"status\": \"finished\",\n  \"collections\": 3,\n  \"itemsDiscovered\": 84,\n  \"candidateItems\": 12,\n  \"indexedItems\": 12,\n  \"skippedNoChange\": 72,\n  \"failed\": 0,\n  \"documentLibraryStats\": {\n    \"candidates\": 9,\n    \"skippedNotNewer\": 6,\n    \"skippedExtNotAllowed\": 3,\n    \"uploadedChunks\": 18\n  }\n}\n</code></pre></p>"},{"location":"ingestion_sharepoint_source/#metrics-reference","title":"Metrics Reference","text":"Counter Meaning Source <code>items_discovered</code> Items enumerated from SharePoint Run summary + App Insights <code>items_candidates</code> Items deemed newer than index Run summary + App Insights <code>items_indexed</code> Body documents uploaded Run summary + App Insights <code>items_skipped_nochange</code> Bodies skipped by freshness Run summary + App Insights <code>items_failed</code> Errors/timeouts Run summary + App Insights <code>body_docs_uploaded</code> Count of body documents uploaded (\u2264 items_indexed) Run summary <code>att_candidates</code> Document-library files considered <code>documentLibraryStats.candidates</code> <code>att_skipped_not_newer</code> Files skipped (index already has newer/equal version) <code>documentLibraryStats.skippedNotNewer</code> <code>att_skipped_ext_not_allowed</code> Files ignored due to extension filter <code>documentLibraryStats.skippedExtNotAllowed</code> <code>att_uploaded_chunks</code> Total chunks pushed for document libraries <code>documentLibraryStats.uploadedChunks</code> <p>Where to find them:</p> <ul> <li> <p>Blob storage: <code>jobs/sharepoint-indexer/runs/latest.json</code> for the most recent run.</p> </li> <li> <p>Application Insights: Query <code>traces</code> (run-level) or <code>customMetrics</code> (time-series) for historical analysis.</p> </li> <li> <p>Dashboards: Combine <code>items_discovered</code>, <code>items_indexed</code>, and <code>documentLibraryStats</code> to visualize workload vs. actual changes each run.</p> </li> </ul>"},{"location":"orchestrator_visual_guide/","title":"Orchestrator: Start Your Code Reading with Visuals","text":"<p>A picture is worth a thousand words. Yet many engineers write another thousand words instead of drawing a single useful diagram. Let\u2019s reverse this evolution \u2014 with visuals.</p> <p>Starting with diagrams \u2014 with a bit of simplification and abstraction \u2014 can significantly accelerate the comprehension of complex codebases. This is especially true when the data flow spans multiple execution environments (container app, Microsoft Foundry, Azure cloud resources), where the initial orientation can otherwise be challenging.</p>"},{"location":"orchestrator_visual_guide/#why-this-article-exists","title":"Why This Article Exists","text":"<p>When I started reading the code, I struggled:</p> <ul> <li>Where is the entry point?</li> <li>What calls what?</li> <li>What is the role of the Orchestrator in the data flow?</li> </ul> <p>If you have ever felt dazed and confused by a codebase with many layers of abstraction, this is for you.</p> <p>If you prefer talking for hours about a diagram instead of drawing it, just leave.</p>"},{"location":"orchestrator_visual_guide/#core-architecture-flow","title":"Core Architecture &amp; Flow","text":"<p>The orchestrator's entry point is in src/main.py: <code>orchestrator_endpoint()</code>. In what follows we will consider the Single-Agent RAG Strategy. <pre><code>@app.post(\n    \"/orchestrator\",\n    dependencies=[Depends(validate_auth)], \n    summary=\"Ask orchestrator a question\",\n    response_description=\"Returns the orchestrator\u2019s response in real time, streamed via SSE.\",\n    responses=ORCHESTRATOR_RESPONSES\n)\nasync def orchestrator_endpoint(\n    body: OrchestratorRequest,\n    x_api_key: Optional[str] = Header(None, alias=\"X-API-KEY\"),\n    dapr_api_token: Optional[str] = Header(None, alias=\"dapr-api-token\"),\n):\n</code></pre></p>"},{"location":"orchestrator_visual_guide/#essential-tasks-of-the-orchestrator","title":"Essential Tasks of the Orchestrator","text":"<p>The <code>Orchestrator</code> class serves as the conversation state manager and strategy coordinator. Its core responsibilities are:</p> <ol> <li>Conversation Lifecycle Management: Creates, loads, and persists conversation documents in the CosmosDB.</li> <li>Strategy Delegation: Routes processing to appropriate agent strategies via factory pattern (<code>AgentStrategyFactory</code>).</li> <li>State Coordination: Ensures conversation state is properly synchronized between database and strategy</li> <li>Response Streaming: Coordinates real-time response delivery while maintaining state consistency</li> </ol>"},{"location":"orchestrator_visual_guide/#single-agent-strategy","title":"Single Agent Strategy","text":"<p>This section explores how the Single-Agent RAG Strategy orchestrates the entire request-response lifecycle, from receiving a user's question to delivering a grounded, streamed answer. The diagrams below illustrate the conversation lifecycle, state management, and the interaction between the Orchestrator container app and Microsoft Foundry services.</p> <p></p> <p>You will have noticed the use of the Factory Design Pattern (<code>AgentStrategyFactory</code>) for the various Strategies, ensuring that all of them comply with the same <code>BaseAgentStrategy</code> interface. For the sake of clarity, I have abstracted away the different roles of the <code>Orchestrator</code> class and the <code>Orchestrator</code> object.</p> <pre><code>------------------------------------------------------------------\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                         Orchestrator                           \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502  - conversation_id: str                                  \u2502  \u2502\n\u2502  \u2502  - database_client: CosmosDBClient     &lt;&lt;reference&gt;&gt;     \u2502  \u2502\n\u2502  \u2502  - agentic_strategy: BaseAgentStrategy \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502  \u2502\n\u2502  \u2502                                                       \u2502  \u2502  \u2502\n\u2502  \u2502  + create()                                           \u2502  \u2502  \u2502\n\u2502  \u2502  + stream_response()                                  \u2502  \u2502  \u2502\n\u2502  \u2502  + save_feedback()                                    \u2502  \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                      \u2502                                    \u2502\n                      \u2502 uses (delegation)                  \u2502 \n                      \u25bc                                    \u2502\n           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                        \u2502  \n           \u2502 AgentStrategyFactory \u2502 &lt;&lt;factory&gt;&gt;            \u2502\n           \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524                        \u2502\n           \u2502 + get_strategy(key)  \u2502                        \u2502\n           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                        \u2502\n                      \u2502                                    \u2502\n                      \u2502 instantiates                       \u2502\n                      \u25bc                                    \u2502\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                    \u2502\n        \u2502    BaseAgentStrategy        \u2502 &lt;&lt;abstract&gt;&gt; \u25c4\u2500\u2500\u2500\u2500\u2500\u2518\n        \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n        \u2502 # strategy_type             \u2502\n        \u2502 # conversation: Dict        \u2502\n        \u2502 # user_context: Dict        \u2502\n        \u2502 # credential                \u2502\n        \u2502 # project_client            \u2502\n        \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n        \u2502 + initiate_agent_flow()*    \u2502 * = abstract method\n        \u2502 # _read_prompt()            \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                       \u2502\n                       \u2502 inheritance (IS-A)\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502             \u2502                         \u2502\n         \u25bc             \u25bc                         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 SingleAgent    \u2502 \u2502  NL2SQL      \u2502  \u2502  McpStrategy    \u2502\n\u2502 RAGStrategy    \u2502 \u2502  Strategy    \u2502  \u2502                 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 - tools_list   \u2502 \u2502 - nl2sql     \u2502  \u2502 - kernel        \u2502\n\u2502 - ai_search    \u2502 \u2502   _plugin    \u2502  \u2502 - agent         \u2502\n\u2502 - event_handler\u2502 \u2502 - terminator \u2502  \u2502                 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 + initiate_    \u2502 \u2502 + initiate_  \u2502  \u2502 + initiate_     \u2502\n\u2502   agent_flow() \u2502 \u2502   agent_flow \u2502  \u2502   agent_flow()  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n------------------------------------------------------------------\n</code></pre> <p>The entry point for the selected Strategy is the method <code>agentic_strategy.initiate_agent_flow()</code>.</p> <p>The strategy object instantiated from the <code>SingleAgentRAGStrategy</code> class runs in the container app and controls the sequence of activities behind the Microsoft Foundry wall. It uses the <code>project_client</code> object as a local proxy (think of it as a remote TV control) to orchestrate operations. The strategy object doesn't handle grounding or LLM calls directly\u2014these are delegated to the Microsoft Foundry agent where the entire RAG pattern is executed.</p> <p>The strategy object <code>SingleAgentRAGStrategy</code></p> <ul> <li> <p>creates a new agent by specifying instructions and a toolbox,</p> </li> <li> <p>retrieves the <code>Thread</code> object based on the thread_id which was retrieved from the CosmosDB,</p> </li> <li> <p>creates a new message from the user's Ask and attaches it to the Thread object,</p> </li> <li> <p>finally calls the project_client.agents.runs.stream() which triggers the RAG pipeline inside of the Microsoft Foundry realm.</p> </li> </ul> <p>Note that <code>Thread</code> objects keep the entire history of conversations. There are two levels of history persistence: one in CosmosDB and another in Thread objects.</p> <p><code>Orchestrator</code> keeps a history (using CosmosDB) identified by <code>conversation_id</code> which arrives in the HTTP Request payload. One of the attributes stored in the CosmosDB is <code>thread_id</code> which points to the <code>Thread</code> object which resides inside of the Microsoft Foundry. Microsoft Foundry maintains its own internal persistency in the Thread objects.</p> <p>The strategy object triggers the RAG pipeline execution inside of Microsoft Foundry with the proxy <code>project_client</code>: <pre><code>project_client.agents.runs.stream(\n    thread_id=thread.id,\n    agent_id=agent.id,\n    ...\n)\n</code></pre> Here is what it does:</p> <ul> <li> <p>Takes the user's Ask.</p> </li> <li> <p>Queries your Azure AI Search index using the <code>AzureAISearchTool</code>.</p> </li> <li> <p>Retrieves relevant document Chunks.</p> </li> <li> <p>Creates the Prompt.</p> </li> <li> <p>Previously retrieved Chunks are included into the Prompt to ground the Response.</p> </li> <li> <p>Prompt is fed into LLM which generates Response.</p> </li> <li> <p>The Response is enhanced by citations and references to the grounding documents.</p> </li> </ul>"},{"location":"orchestrator_visual_guide/#single-agent-strategy-internal-flow","title":"Single Agent Strategy Internal Flow","text":"<p>The sequence diagram above is intended to illustrate the core concepts and design patterns present in the codebase. The visualization deliberately simplifies reality through abstraction and by omitting less relevant details.</p>"},{"location":"orchestrator_visual_guide/#links-to-the-code","title":"Links to the code","text":"Concept File Notes Orchestrator entry <code>main.py</code> FastAPI route + request handling Orchestrator implementation <code>orchestration/orchestrator.py</code> Maintains Conversation History + runs streaming pipeline Strategy factory <code>strategies/agent_strategy_factory.py</code> Selects the execution strategy Single-Agent RAG Strategy <code>strategies/single_agent_rag_strategy.py</code> Implements flow to Azure Microsoft Foundry"},{"location":"quickstart_nl2sql/","title":"NL2SQL Quick Start Guide","text":"<p>Get public access natural language querying of your Azure SQL Database working in 30 minutes using automated blob storage ingestion.</p> <p>This quickstart enables a public access solution which is for testing purposes only!</p> <p>Prerequisites</p> <p>You must have:</p> <ul> <li>GPT-RAG solution deployed (<code>azd provision</code> and <code>azd deploy</code> completed)</li> <li>Azure subscription with permissions to create SQL resources</li> <li>Access to Azure Portal</li> </ul> <p>What You'll Accomplish</p> <p>By the end of this guide:</p> <ul> <li>Create Azure SQL Database with AdventureWorksLT sample data</li> <li>Configure networking and firewall rules</li> <li>Store database credentials securely in Key Vault</li> <li>Register your SQL database as a datasource</li> <li>Upload table metadata and example queries to blob storage</li> <li>Enable automated ingestion with CRON scheduling</li> <li>Enable NL2SQL strategy</li> <li>Ask questions in natural language and get SQL results</li> </ul> <p>Step 1: Create Azure SQL Database (5-10 min)</p> <p>A. Create SQL Server</p> <p>Azure Portal \u2192 Create a resource \u2192 SQL Database</p> <ol> <li> <p>Basics tab:</p> <ul> <li>Subscription: Your subscription</li> <li>Resource group: Same as your GPT-RAG deployment (or create new)</li> <li>Database name: <code>adventureworks-demo</code></li> <li>Server: Click Create new</li> </ul> </li> <li> <p>Create SQL Database Server:</p> <ul> <li>Server name: <code>sql-gptrag-demo-&lt;unique&gt;</code> (must be globally unique)</li> <li>Location: Same region as GPT-RAG (optional - any region works, but same region reduces latency)</li> <li>Authentication method: Use SQL authentication (simpler for this quickstart; Entra ID authentication is also supported)</li> <li>Server admin login: <code>sqladmin</code></li> <li>Password: Create a strong password (save this!)</li> <li>Click OK</li> </ul> </li> <li> <p>Compute + storage:</p> <ul> <li>Click Configure database</li> <li>Select Basic (cheapest for testing - $5/month)</li> <li>Click Apply</li> </ul> </li> <li> <p>Backup storage redundancy:</p> <ul> <li>Select Locally-redundant backup storage (cheapest)</li> </ul> </li> </ol> <p>B. Configure Networking</p> <ol> <li>Networking tab:<ul> <li>Connectivity method: Public endpoint</li> <li>Firewall rules:</li> <li>Allow Azure services and resources to access this server - YES (CRITICAL!)</li> <li>Add current client IP address - YES (for your testing)</li> <li>Connection policy: Default</li> <li>Encrypted connections: TLS 1.2 (default)</li> </ul> </li> </ol> <p>C. Add Sample Data</p> <ol> <li> <p>Additional settings tab:</p> <ul> <li>Use existing data: Sample (AdventureWorksLT)</li> <li>Collation: Default</li> <li>Enable Microsoft Defender: Not needed for demo</li> </ul> </li> <li> <p>Click Review + create \u2192 Create</p> </li> </ol> <p>\u23f3 Wait 3-5 minutes for deployment to complete.</p> <p>D. Verify Firewall Configuration (Post-Deployment)</p> <p>After deployment completes:</p> <p>\u26a0\ufe0f Important: Go to the SQL Server resource (not the database):</p> <p>Azure Portal \u2192 SQL servers \u2192 <code>sql-gptrag-demo-&lt;unique&gt;</code> \u2192 Security \u2192 Networking</p> <p>(Note: SQL databases \u2192 adventureworks-demo - that's the wrong place!)</p> <p>Verify these settings:</p> <ul> <li>Public network access: Selected networks</li> <li>Exceptions: \u2611\ufe0f Allow Azure services and resources to access this server (checked)</li> </ul> <p>The \"Allow Azure services\" checkbox creates a special firewall rule (<code>0.0.0.0 - 0.0.0.0</code>) that permits any Azure service in your subscription to connect.</p> <p>E. Test Connection</p> <p>Using Azure Portal Query Editor:</p> <ol> <li>Go to your SQL Database \u2192 Query editor</li> <li>Login with SQL authentication (sqladmin / your password)</li> <li>Run test query:     <pre><code>SELECT TOP 5 ProductID, Name, ListPrice \nFROM SalesLT.Product \nORDER BY ListPrice DESC\n</code></pre></li> </ol> <p>If you see results, your database is ready!</p> <p>F. Gather Connection Info</p> <p>Save these details (you'll need them later):</p> <pre><code>Server: &lt;your-sql-server-name&gt;.database.windows.net\nDatabase: adventureworks-demo\nUsername: sqladmin\nPassword: &lt;your-password&gt;\n</code></pre> <p>Step 2: Find Your GPT-RAG Resources (3 min)</p> <p>Locate these in your resource group (they have your deployment suffix):</p> <pre><code>Key Vault: kv-&lt;suffix&gt;\nCosmos DB: cosmos-&lt;suffix&gt;\nAI Search: srch-&lt;suffix&gt;\nApp Config: appcs-&lt;suffix&gt;\nStorage Account: st&lt;suffix&gt; (note: no dash in storage account names)\n</code></pre> <p>Quick way to find suffix: <pre><code># List resource groups\naz group list --query \"[?contains(name,'gpt-rag')].name\" -o table\n\n# List resources in your group\naz resource list -g &lt;your-rg&gt; --query \"[].name\" -o table\n</code></pre></p> <p>Step 3: Store Password in Key Vault (3 min)</p> <p>CRITICAL: Secret name MUST be <code>{datasource-id}-secret</code></p> <pre><code># Using the password you created in Step 1\n# If your datasource id will be \"adventureworks\"\n# Then secret name must be \"adventureworks-secret\"\n\naz keyvault secret set `\n  --vault-name \"kv-&lt;your-suffix&gt;\" `\n  --name \"adventureworks-secret\" `\n  --value \"&lt;your-sql-password-from-step-1&gt;\"\n</code></pre> <p>Verify it worked: <pre><code>az keyvault secret show `\n  --vault-name \"kv-&lt;your-suffix&gt;\" `\n  --name \"adventureworks-secret\" `\n  --query \"value\" -o tsv\n</code></pre></p> <p>Step 4: Grant Container App Access to Key Vault (2 min)</p> <pre><code># Get orchestrator's managed identity\n$principalId = az containerapp show `\n  --name \"ca-&lt;your-suffix&gt;-orchestrator\" `\n  --resource-group &lt;your-rg&gt; `\n  --query \"identity.principalId\" -o tsv\n\n# Grant access\naz role assignment create `\n  --assignee $principalId `\n  --role \"Key Vault Secrets User\" `\n  --scope \"/subscriptions/&lt;sub-id&gt;/resourceGroups/&lt;rg&gt;/providers/Microsoft.KeyVault/vaults/kv-&lt;your-suffix&gt;\"\n</code></pre> <p>Step 5: Register Database in Cosmos DB (3 min)</p> <p>Azure Portal \u2192 Cosmos DB \u2192 cosmos-<code>&lt;suffix&gt;</code> \u2192 Data Explorer \u2192 ragdata database \u2192 datasources container \u2192 New Item</p> <p>Paste this JSON, replacing <code>&lt;your-sql-server-name&gt;</code> with the server name you created in Step 1 (e.g., <code>sql-gptrag-demo-xyz123</code>):</p> <pre><code>{\n    \"id\": \"adventureworks\",\n    \"type\": \"sql_database\",\n    \"description\": \"AdventureWorksLT sample database with products and customers\",\n    \"server\": \"&lt;your-sql-server-name&gt;.database.windows.net\",\n    \"database\": \"adventureworks-demo\",\n    \"uid\": \"sqladmin\",\n    \"metadata\": {\n        \"created_date\": \"2025-11-17\"\n    }\n}\n</code></pre> <p>\ud83d\udca1 How to find your server name:</p> <ul> <li>Azure Portal \u2192 SQL servers \u2192 Look for the server you just created</li> <li>Copy the name (e.g., <code>sql-gptrag-demo-ragpace</code>)</li> <li>Add <code>.database.windows.net</code> to the end</li> </ul> <p>\u26a0\ufe0f CRITICAL RULES:</p> <ul> <li>Use <code>uid</code> (NOT <code>username</code>)</li> <li>DO NOT include <code>password</code> field</li> <li>DO NOT include <code>connection_info</code> field</li> <li>The <code>id</code> must match Key Vault secret prefix (<code>adventureworks</code> \u2192 <code>adventureworks-secret</code>)</li> </ul> <p>Click Save.</p> <p>Step 6: Create Metadata JSON Files (5 min)</p> <p>What you'll do in this step:</p> <ul> <li>Create JSON files on your local machine (in your working directory)</li> <li>These files describe your database tables and example queries</li> <li>In Step 7, you'll upload these files to Azure Blob Storage</li> <li>The automated ingestion system will then index them into AI Search</li> </ul> <p>A. Create Local Folder Structure</p> <p>On your local machine, create folders to organize the metadata files:</p> <pre><code>mkdir blob-upload\nmkdir blob-upload\\queries\nmkdir blob-upload\\tables\n</code></pre> <p>B. Create Example Query Files</p> <p>Why example queries? </p> <p>They help the AI understand your database patterns and generate better SQL. The system uses these as few-shot examples when translating natural language to SQL.</p> <p>\ud83d\udcdd Note: We're only adding 3 queries here for quick setup. Ideally, add 10-20 diverse examples covering:</p> <ul> <li>Simple queries (counts, filters)</li> <li>Complex joins across multiple tables</li> <li>Aggregations (SUM, AVG, GROUP BY)</li> <li>Date/time filtering</li> <li>Common business questions your users ask </li> </ul> <p>More examples = better SQL generation accuracy!</p> <p>Create these files on your local machine:</p> <p>Create <code>blob-upload\\queries\\how_many_products.json</code>: <pre><code>{\n    \"question\": \"How many products are in the database?\",\n    \"query\": \"SELECT COUNT(*) as product_count FROM SalesLT.Product\",\n    \"reasoning\": \"Simple count of all products\",\n    \"datasource\": \"adventureworks\"\n}\n</code></pre></p> <p>Create <code>blob-upload\\queries\\top_expensive_products.json</code>: <pre><code>{\n    \"question\": \"Show me the top 5 most expensive products\",\n    \"query\": \"SELECT TOP 5 ProductID, Name, ListPrice FROM SalesLT.Product ORDER BY ListPrice DESC\",\n    \"reasoning\": \"Get highest priced products\",\n    \"datasource\": \"adventureworks\"\n}\n</code></pre></p> <p>Create <code>blob-upload\\queries\\product_categories.json</code>: <pre><code>{\n    \"question\": \"What product categories exist?\",\n    \"query\": \"SELECT DISTINCT Name FROM SalesLT.ProductCategory ORDER BY Name\",\n    \"reasoning\": \"List all unique categories\",\n    \"datasource\": \"adventureworks\"\n}\n</code></pre></p> <p>C. Create Table Metadata Files</p> <p>Why table metadata? </p> <p>The system uses AI Search for semantic table discovery instead of live database introspection. When users ask questions, the system:</p> <ol> <li>Searches your table metadata using embeddings (vector search)</li> <li>Finds the most relevant tables based on descriptions</li> <li>Then calls <code>GetSchemaInfo</code> to retrieve detailed column information</li> </ol> <p>Benefits of this approach:</p> <ul> <li>Fast semantic search - Find relevant tables using natural language (\"revenue data\" matches \"SalesOrderHeader\")</li> <li>Control what's exposed - Only include tables relevant to end users (exclude admin/audit tables)</li> <li>Add business context - Descriptions help the AI understand table purpose beyond raw schema</li> <li>Avoid token limits - Don't send 500 table schemas to GPT-4 every query</li> </ul> <p>\u26a0\ufe0f Schema updates:</p> <ul> <li>If you add/drop columns or tables later, create new JSON files and upload them</li> <li>The automated ingestion will detect changes and re-index automatically</li> </ul> <p>Note</p> <p>Column descriptions are not specified here because the column names are sufficiently descriptive for the LLM</p> <p>Create these files on your local machine:</p> <p>Create <code>blob-upload\\tables\\saleslt_product.json</code>: <pre><code>{\n    \"table\": \"SalesLT.Product\",\n    \"description\": \"Product catalog with names, prices, and descriptions\",\n    \"datasource\": \"adventureworks\"\n}\n</code></pre></p> <p>Create <code>blob-upload\\tables\\saleslt_productcategory.json</code>: <pre><code>{\n    \"table\": \"SalesLT.ProductCategory\",\n    \"description\": \"Product categories for organizing products\",\n    \"datasource\": \"adventureworks\"\n}\n</code></pre></p> <p>Create <code>blob-upload\\tables\\saleslt_customer.json</code>: <pre><code>{\n    \"table\": \"SalesLT.Customer\",\n    \"description\": \"Customer information including names and contact details\",\n    \"datasource\": \"adventureworks\"\n}\n</code></pre></p> <p>Step 7: Upload Files to Blob Storage (3 min)</p> <p>What this step does: - Uploads the local JSON files you created in Step 6 to Azure Blob Storage - Files go into the <code>nl2sql</code> container (this container already exists from your deployment) - The folder structure (<code>queries/</code> and <code>tables/</code>) will be created automatically during upload</p> <p>Upload command:</p> <pre><code># Upload all files (queries and tables)\naz storage blob upload-batch `\n  --account-name \"st&lt;your-suffix&gt;\" `\n  --destination nl2sql `\n  --source blob-upload `\n  --auth-mode login\n</code></pre> <p>Verify uploads: <pre><code># List all blobs in nl2sql container\naz storage blob list `\n  --account-name \"st&lt;your-suffix&gt;\" `\n  --container-name nl2sql `\n  --auth-mode login `\n  --query \"[].name\" -o table\n</code></pre></p> <p>You should see: <pre><code>queries/how_many_products.json\nqueries/top_expensive_products.json\nqueries/product_categories.json\ntables/saleslt_product.json\ntables/saleslt_productcategory.json\ntables/saleslt_customer.json\n</code></pre></p> <p>Step 8: Enable Automated Ingestion (3 min)</p> <p>Azure Portal \u2192 App Configuration \u2192 appcs-<code>&lt;suffix&gt;</code> \u2192 Configuration explorer</p> <p>Click + Create to add a new key-value:</p> <p>Key: <code>CRON_RUN_NL2SQL_INDEX</code> Value: <code>*/15 * * * *</code> Label: <code>gpt-rag-ingestion</code> Content type: <code>text/plain</code></p> <p>Click Apply.</p> <p>What this does:</p> <ul> <li>Runs the NL2SQL indexer job every 15 minutes</li> <li>Scans the <code>nl2sql</code> container for new/changed files</li> <li>Automatically indexes them into AI Search</li> <li>Generates embeddings for semantic search</li> <li>Skips unchanged files (smart change detection)</li> </ul> <p>Alternative schedules:</p> <ul> <li><code>*/5 * * * *</code> - Every 5 minutes (faster updates)</li> <li><code>*/2 * * * *</code> - Every 2 minutes (testing/development only)</li> <li><code>0 * * * *</code> - Every hour on the hour</li> <li><code>0 0 * * *</code> - Once daily at midnight</li> </ul> <p>Cost considerations:</p> <ul> <li>CRON schedules are free - they're just timers, not separate compute</li> <li>The Container App runs 24/7 regardless of schedule (~$30-50/month)</li> <li>Each run generates embeddings via Azure OpenAI (~$0.001 per run)</li> <li>Smart change detection skips unchanged files (saves costs)</li> <li>Recommended for production: <code>*/15 * * * *</code> balances responsiveness with costs (~$3-5/month in embeddings)</li> <li>For testing: Use <code>*/2 * * * *</code> to see results faster, then change to <code>*/15 * * * *</code></li> </ul> <p>Step 9: Enable NL2SQL Strategy (2 min)</p> <p>Azure Portal \u2192 App Configuration \u2192 appcs-<code>&lt;suffix&gt;</code> \u2192 Configuration explorer</p> <ol> <li>Search for key: <code>AGENT_STRATEGY</code></li> <li>Click the key \u2192 Click Edit</li> <li>Change value to: <code>nl2sql</code></li> <li>Click Apply</li> </ol> <p>Step 10: Wait for Initial Ingestion (2-3 min)</p> <p>The data ingestion Container App runs the indexer job:</p> <ul> <li>On startup (happens once when container starts)</li> <li>Every 15 minutes (based on CRON schedule)</li> </ul> <p>Option 1: Wait for next CRON run (up to 15 minutes, or 2 minutes if you used <code>*/2 * * * *</code> for testing)</p> <p>How it works:</p> <ul> <li>The Container App runs the indexer on startup (immediate first run)</li> <li>Then runs on the CRON schedule you configured</li> <li>CRON runs at the next matching time (e.g., if schedule is <code>*/15 * * * *</code>, next run is at :00, :15, :30, or :45)</li> </ul> <p>Option 2: Force immediate ingestion by restarting the container:</p> <pre><code>az containerapp revision restart `\n  --name \"ca-&lt;your-suffix&gt;-dataingest\" `\n  --resource-group &lt;your-rg&gt; `\n  --revision $(az containerapp revision list --name \"ca-&lt;your-suffix&gt;-dataingest\" --resource-group &lt;your-rg&gt; --query \"[0].name\" -o tsv)\n</code></pre> <p>Verify indexing completed:</p> <p>Check the logs in blob storage: <pre><code># List ingestion run logs\naz storage blob list `\n  --account-name \"st&lt;your-suffix&gt;\" `\n  --container-name jobs `\n  --prefix \"nl2sql-indexer/runs/\" `\n  --auth-mode login `\n  --query \"[].{name:name, modified:properties.lastModified}\" -o table\n</code></pre></p> <p>Download the most recent log: <pre><code># Get the latest run log\n$latestLog = az storage blob list `\n  --account-name \"st&lt;your-suffix&gt;\" `\n  --container-name jobs `\n  --prefix \"nl2sql-indexer/runs/\" `\n  --auth-mode login `\n  --query \"[-1].name\" -o tsv\n\n# Download and view it\naz storage blob download `\n  --account-name \"st&lt;your-suffix&gt;\" `\n  --container-name jobs `\n  --name $latestLog `\n  --file run-log.json `\n  --auth-mode login\n\nGet-Content run-log.json | ConvertFrom-Json | ConvertTo-Json -Depth 10\n</code></pre></p> <p>Look for:</p> <ul> <li><code>\"success\": 3</code> (for queries)</li> <li><code>\"success\": 3</code> (for tables)</li> <li><code>\"failed\": 0</code></li> <li><code>\"skipped\": 0</code> (first run - nothing skipped yet)</li> <li><code>\"vectorsGenerated\": 6</code> (embeddings created)</li> </ul> <p>On subsequent runs:</p> <ul> <li>Files with no changes will show <code>\"skipped\": 6</code> and <code>\"candidates\": 0</code></li> <li>Only new or modified files will be re-indexed</li> <li>This smart change detection saves costs and time</li> </ul> <p>Step 11: Test It! (2 min)</p> <p>Navigate to your UI: <code>https://ca-&lt;suffix&gt;-frontend.livelyglacier-&lt;random&gt;.eastus2.azurecontainerapps.io</code></p> <p>Try these questions:</p> <ol> <li>\"How many products are in the database?\"</li> <li>\"Show me the top 5 most expensive products\"</li> <li>\"What product categories exist?\"</li> </ol> <p>Expected response:</p> <ul> <li>Natural language answer with data</li> <li>Shows SQL query that was executed</li> <li>Cites the datasource</li> </ul> <p>Congratulations! You've set up an automated NL2SQL ingestion pipeline.</p> <p>What You Just Enabled \ud83e\ude84</p> <p>\ud83e\udde0 Automated Metadata Management</p> <p><code>Change detection</code> - Only re-indexes modified files (checks ETag and lastModified)</p> <p><code>Smart scheduling</code> - Runs every 15 minutes, can be adjusted</p> <p><code>Startup sync</code> - Runs once on container startup for immediate availability</p> <p><code>Detailed logging</code> - Per-file and per-run logs in blob storage</p> <p><code>Scalable</code> - Handles hundreds of tables and queries efficiently</p> <p>\ud83e\udde9 Advanced Query Capabilities (Already working!)</p> <p><code>Complex JOINs</code> - \"Show me orders with customer names and product details\" automatically generates multi-table joins</p> <p><code>Aggregations</code> - \"What's the average order value by product category?\" generates GROUP BY with AVG/SUM/COUNT</p> <p><code>Date filtering</code> - \"Show orders from last 30 days\" uses DATEADD and date functions</p> <p><code>Subqueries</code> - Handles nested queries when needed for complex business logic</p> <p><code>Pattern matching</code> - \"Find customers whose email contains 'adventure'\" uses LIKE operators</p> <p>\ud83d\udd10 Enabling Zero Trust Architecture</p> <p>The guide used SQL authentication for simplicity, but production deployments support:</p> <p><code>Azure AD authentication</code> - No passwords, just managed identities</p> <p><code>Private Endpoints</code> - Database never exposed to internet</p> <p><code>VNet integration</code> - Container Apps and SQL in same private network</p>"},{"location":"quickstart_simple_rag/","title":"Simple RAG Quick Start Guide","text":"<p>Get document-based AI question answering working in 60 minutes using GPT-RAG's automated deployment.</p> <p>This quickstart enables a public access solution which is for testing purposes only!</p> <p>Prerequisites</p> <p>You must have:</p> <ul> <li>Azure subscription with permissions to create resources</li> <li>Azure Developer CLI (azd): 1.21.1+</li> <li>Azure CLI (az): 2.79.0+</li> <li>PowerShell 7.5+</li> <li>Docker Desktop installed and running</li> <li>Git &amp; Python 3.11+</li> </ul> <p>What You'll Accomplish</p> <p>By the end of this guide:</p> <ul> <li>Deploy complete GPT-RAG infrastructure (~20 Azure resources)</li> <li>Upload documents to Azure Blob Storage</li> <li>Enable automated document ingestion and indexing</li> <li>Configure AI Foundry search connection</li> <li>Generate embeddings with text-embedding-3-large</li> <li>Ask questions about your documents using GPT-4o</li> <li>Get answers grounded in your data with citations</li> </ul> <p>Step 1: Initialize GPT-RAG Project (5 min)</p> <pre><code># Create fresh directory\nmkdir C:\\MyCode\\my-gpt-rag\ncd C:\\MyCode\\my-gpt-rag\n\n# Initialize from template\nazd init -t azure/gpt-rag\n</code></pre> <p>You'll be prompted for:</p> <ol> <li>Environment name - Choose a short name (e.g., <code>myrag</code>, <code>demorag</code>)</li> <li>Azure subscription - Select from list</li> <li>Azure region - Choose wisely:<ul> <li>East US 2 - Best OpenAI quota availability</li> <li>West US - Good quota, lower latency for West Coast</li> <li>East US - Backup option if East US 2 full</li> <li>\u26a0\ufe0f North Europe, West Europe - Often quota-constrained</li> </ul> </li> </ol> <p>\ud83d\udca1 Note: If you have quota limitations, see the Troubleshooting section at the end for how to adjust model capacity.</p> <p>Step 2: Authenticate Azure CLIs (2 min)</p> <pre><code># Login to Azure CLI (yes, both required)\naz login\nazd auth login\n</code></pre> <p>Step 3: Provision Infrastructure (30 min)</p> <pre><code>azd provision\n</code></pre> <p>What this creates (~20 Azure resources):</p> <ul> <li>Resource Group - Container for all resources</li> <li>Storage Accounts - For documents and job logs</li> <li>AI Search Services (2) - Main index (srch-) and AI Foundry index (srch-aif-)</li> <li>AI Foundry - Hub and project for agent orchestration</li> <li>OpenAI Models - GPT-4o and text-embedding-3-large deployments</li> <li>Container Registry - Stores Docker images</li> <li>Container Apps (4) - Frontend, orchestrator, ingestion, MCP</li> <li>App Configuration - Centralized configuration store</li> <li>Key Vault - Secrets management</li> <li>Cosmos DB - Agent state and metadata</li> <li>Log Analytics - Monitoring and diagnostics</li> <li>Managed Identities - Secure service-to-service auth</li> </ul> <p>\u23f3 Wait ~27 minutes for provisioning to complete.</p> <p>Save your resource prefix:</p> <p>After provisioning completes, you'll see output like: <pre><code>Resource prefix: y5sbzlaazfxok\n</code></pre></p> <p>Keep this prefix handy - you'll use it to identify your resources in Azure Portal.</p> <p>Step 4: Start Docker and Deploy Services (35 min)</p> <p>\u26a0\ufe0f CRITICAL: Docker must be running before <code>azd deploy</code>!</p> <p>A. Start Docker Desktop</p> <p>B. Deploy Services</p> <pre><code>azd deploy\n</code></pre> <p>What this does:</p> <ol> <li> <p>Clones 4 service repositories from GitHub:</p> <ul> <li><code>gpt-rag-frontend</code> - React web interface</li> <li><code>gpt-rag-orchestrator</code> - Agent orchestration and RAG logic</li> <li><code>gpt-rag-ingestion</code> - Document processing and indexing</li> <li><code>gpt-rag-mcp</code> - Model Context Protocol integration</li> </ul> </li> <li> <p>Builds Docker images locally:</p> <ul> <li>Creates containers for each service</li> <li>Tags them with version info</li> <li>Requires ~5-10 GB disk space</li> </ul> </li> <li> <p>Pushes images to Azure Container Registry:</p> <ul> <li>Uploads to <code>cr{prefix}.azurecr.io</code></li> <li>Typically 1-2 GB total</li> </ul> </li> <li> <p>Deploys to Container Apps:</p> <ul> <li>Creates 4 container apps</li> <li>Configures networking, secrets, environment variables</li> <li>Sets up auto-scaling rules</li> </ul> </li> </ol> <p>\u23f3 Wait ~30 minutes for deployment to complete.</p> <p>Step 5: Upload Documents and Verify Indexing (10 min)</p> <p>A. Upload Your Documents</p> <p>Azure Portal \u2192 Storage accounts \u2192 st<code>&lt;prefix&gt;</code> \u2192 Containers \u2192 documents \u2192 Upload</p> <p>Supported formats:</p> <ul> <li>PDF - Most common (OCR with Document Intelligence)</li> <li>Word (.docx) - Extracts text and tables</li> <li>PowerPoint (.pptx) - Extracts slides and speaker notes</li> <li>Excel (.xlsx) - Extracts sheets and data (v4.0+)</li> <li>Text (.txt, .md) - Direct text ingestion</li> <li>Images (.jpg, .png, .bmp, .tiff) - OCR extraction</li> <li>HTML - Web page content extraction</li> </ul> <p>What happens after upload:</p> <ol> <li>File stored in <code>documents</code> container</li> <li>Ingestion service monitors for new files</li> <li>On next CRON run or restart, files are processed</li> <li>Azure Document Intelligence extracts text (OCR for images/PDFs)</li> <li>Text chunked into semantic segments</li> <li>Embeddings generated with text-embedding-3-large</li> <li>Chunks uploaded to AI Search index <code>ragindex-{prefix}</code></li> </ol> <p>B. Trigger Document Indexing</p> <p>The indexing service runs automatically:</p> <ul> <li>On container startup (immediate first run)</li> <li>Hourly on CRON (at :10 minutes past the hour)</li> </ul> <p>Option 1: Wait for next CRON run (up to 60 minutes)</p> <p>Option 2: Force immediate indexing by restarting:</p> <p>Azure Portal \u2192 Container Apps \u2192 ca-<code>&lt;prefix&gt;</code>-dataingest</p> <ol> <li>Click Stop</li> <li>Wait 10 seconds</li> <li>Click Start</li> </ol> <p>Why restart? The indexer runs on CRON schedule (hourly at :10 past). Restarting triggers it to run immediately instead of waiting up to an hour.</p> <p>C. Verify Indexing Completed</p> <p>Azure Portal \u2192 AI Search \u2192 srch-<code>&lt;prefix&gt;</code> \u2192 Indexes \u2192 ragindex-<code>&lt;prefix&gt;</code></p> <ul> <li>Document count &gt; 0 (indicates chunks were indexed)</li> </ul> <p>Step 6: Access Frontend and Test! (3 min)</p> <p>Azure Portal \u2192 Container Apps \u2192 ca-<code>&lt;prefix&gt;</code>-frontend \u2192 Overview</p> <p>Copy the Application URL and open it in your browser. You should see the GPT-RAG chat interface.</p> <p>Try these test questions:</p> <ol> <li>\"What is this document about?\"</li> <li>\"Summarize the key points from my document\"</li> <li>\"What does the document say about [specific topic]?\"</li> </ol> <p>Expected response:</p> <ul> <li>Natural language answer based on your document</li> <li>Citations showing which chunks were used</li> <li>Source references with page numbers (if PDF)</li> <li>Response generated by GPT-4o using retrieved context</li> </ul> <p>Troubleshooting Common Issues</p> <p>Issue 1: Quota Exceeded Errors</p> <p>Error during provision: <pre><code>Deployment failed: Quota exceeded for model gpt-4o\n</code></pre></p> <p>Background: The default deployment requires 80 TPM total (40 for GPT-4o + 40 for text-embedding-3-large). Many subscriptions don't have this quota available.</p> <p>Solution A: Reduce Model Capacity</p> <p>Edit <code>infra/main.parameters.json</code> BEFORE running <code>azd provision</code>:</p> <pre><code>\"modelDeploymentList\": {\n  \"value\": [\n    {\n      \"name\": \"chat\",\n      \"model\": {\n        \"format\": \"OpenAI\",\n        \"name\": \"gpt-4o\",\n        \"version\": \"2024-11-20\"\n      },\n      \"sku\": {\n        \"name\": \"GlobalStandard\",\n        \"capacity\": 8\n      }\n    },\n    {\n      \"name\": \"text-embedding\",\n      \"model\": {\n        \"format\": \"OpenAI\",\n        \"name\": \"text-embedding-3-large\",\n        \"version\": \"1\"\n      },\n      \"sku\": {\n        \"name\": \"Standard\",\n        \"capacity\": 8\n      }\n    }\n  ]\n}\n</code></pre> <p>Capacity guidelines:</p> <ul> <li>8 TPM - Good for testing/demos (supports ~5-10 users)</li> <li>16 TPM - Better for development (supports ~15-20 users)</li> <li>40 TPM - Production (default, supports ~50+ users)</li> <li>Minimum: 4 TPM - Will work but responses may be slower</li> </ul> <p>Solution B: Check Available Quota</p> <pre><code># List your OpenAI resources and regions\naz cognitiveservices account list `\n  --subscription \"&lt;your-subscription-id&gt;\" `\n  --query \"[?kind=='OpenAI'].{Name:name, ResourceGroup:resourceGroup, Location:location}\" `\n  -o table\n</code></pre> <p>Check quota in Azure Portal: - Go to any OpenAI resource \u2192 Quotas - Look at TPM (Tokens Per Minute) for gpt-4o and text-embedding-3-large - Note which regions have available quota</p> <p>Solution C: Try Different Region</p> <p>If you've already started provisioning, delete the failed resource group and try a different region:</p> <pre><code>az group delete --name &lt;your-rg&gt; --yes\nazd env set AZURE_LOCATION eastus\nazd provision\n</code></pre> <p>Issue 2: \"ServiceInvocationException\" Error</p> <p>Error message: <pre><code>ServiceInvocationException: Error at: project_client.agents.threads.create()\n</code></pre></p> <p>Root cause: AI Foundry capabilities host lacks vector store connections (immutable configuration).</p> <p>Workaround:</p> <ol> <li>Check orchestrator logs: <code>ca-&lt;prefix&gt;-orchestrator</code> \u2192 Logs \u2192 Console logs</li> <li>Verify SEARCH_CONNECTION_ID is correctly set in App Configuration</li> <li>Try restarting orchestrator again</li> <li>If persists, check AI Foundry Connections page for connection status</li> </ol> <p>Note: This is a known issue with the current GPT-RAG template. The AI Foundry agent infrastructure may need manual configuration.</p> <p>Issue 3: No Documents Indexed</p> <p>Symptoms:</p> <ul> <li>AI Search index shows 0 documents</li> <li>Answers are generic, not grounded in your data</li> </ul> <p>Checks:</p> <ol> <li>Verify upload: Storage account \u2192 <code>documents</code> container \u2192 your file should be there</li> <li>Check indexer logs: <code>ca-&lt;prefix&gt;-dataingest</code> \u2192 Logs \u2192 Look for errors</li> <li>Verify format: Ensure file is supported (PDF, DOCX, TXT, etc.)</li> <li>Restart indexer: Deactivate/Activate <code>ca-&lt;prefix&gt;-dataingest</code></li> </ol> <p>Congratulations! You've deployed a production-ready RAG system. </p> <p>What You Just Enabled \ud83e\ude84</p> <p>\ud83e\udde0 Intelligent Document Understanding</p> <p><code>Semantic chunking</code> - Documents split into meaningful segments (not arbitrary 512-char blocks)</p> <p><code>Vector embeddings</code> - text-embedding-3-large generates 3072-dimensional embeddings</p> <p><code>Hybrid search</code> - Combines keyword (BM25) and semantic (vector) search for best results</p> <p><code>OCR extraction</code> - Azure Document Intelligence handles PDFs, images, scanned documents</p> <p><code>Multi-format support</code> - PDF, Word, PowerPoint, Excel, text, images, HTML</p> <p>\ud83c\udfaf Grounded Response Generation</p> <p><code>Retrieval-Augmented Generation (RAG)</code> - GPT-4o generates answers using retrieved document chunks</p> <p><code>Citation tracking</code> - Every answer links back to source documents and specific passages</p> <p><code>Relevance filtering</code> - Only high-confidence matches are sent to GPT-4o (reduces hallucination)</p> <p><code>Context windowing</code> - Smart selection of most relevant chunks (handles long documents)</p> <p>\ud83d\udd04 Automated Ingestion Pipeline</p> <p><code>Change detection</code> - Only re-indexes modified files (checks ETag and lastModified)</p> <p><code>CRON scheduling</code> - Runs hourly at :10 past (configurable)</p> <p><code>Startup sync</code> - Indexes on container start for immediate availability</p> <p><code>Detailed logging</code> - Per-file and per-run logs in blob storage</p> <p><code>Error handling</code> - Failed files logged without blocking successful files</p> <p>\ud83d\udd10 Enterprise-Ready Architecture</p> <p><code>Managed identities</code> - No passwords stored, secure service-to-service auth</p> <p><code>Role-based access</code> - Uses Azure RBAC for all resources</p> <p><code>Centralized config</code> - App Configuration for runtime settings (no redeployment needed)</p> <p><code>Secrets management</code> - Key Vault for sensitive data</p> <p><code>Monitoring</code> - Log Analytics and Application Insights built-in</p> <p><code>Scalability</code> - Container Apps auto-scale based on load</p> <p>\ud83c\udfa8 User-Friendly Interface</p> <p><code>Web UI</code> - Modern, responsive chat interface</p> <p><code>Real-time responses</code> - Streaming responses </p> <p><code>Citation preview</code> - Click citations to see source context</p> <p><code>Multi-turn conversations</code> - Maintains context across questions</p> <p><code>Feedback loop</code> - Users can rate answers for quality tracking</p>"},{"location":"services_ingestion/","title":"Overview","text":"<p>The GPT-RAG Data Ingestion service automates the processing of diverse document types, such as PDFs, images, spreadsheets, transcripts, and SharePoint files, preparing them for indexing in Azure AI Search. It uses intelligent chunking strategies tailored to each format, generates text and image embeddings, and enables rich, multimodal retrieval experiences for agent-based RAG applications.</p>"},{"location":"services_ingestion/#key-features","title":"Key Features","text":"<ul> <li>Multi-Format Processing: Handles PDFs, images, spreadsheets, transcripts, and SharePoint content</li> <li>Intelligent Chunking: Format-specific chunking strategies for optimal retrieval</li> <li>Multimodal Embeddings: Generates both text and image embeddings</li> <li>Automated Workflows: Scans sources, processes content, and indexes documents automatically</li> <li>Scheduled Execution: CRON-based scheduler for continuous data ingestion</li> <li>Multiple Data Sources: Supports Blob Storage, SharePoint, and NL2SQL metadata</li> </ul>"},{"location":"services_ingestion/#data-sources","title":"Data sources","text":"<ul> <li>Blob Storage</li> <li>NL2SQL Metadata</li> <li>SharePoint</li> </ul>"},{"location":"services_ingestion/#how-to-deploy-the-data-ingestion-service","title":"How to deploy the data ingestion service","text":"<p>Prerequisites</p> <p>Provision the infrastructure first by following the Deployment Guide. This ensures all required Azure resources (e.g., Container App, Storage, AI Search) are in place before deploying the data ingestion service.</p> <p>Required Tools:</p> <ul> <li>Azure CLI</li> <li>Azure Developer CLI (optional, if using azd)</li> <li>Git</li> <li>Python 3.12</li> <li>Docker CLI</li> <li>VS Code (recommended)</li> </ul> <p>Required Permissions (for customization):</p> Resource Role Description App Configuration Store App Configuration Data Owner Full control over configuration settings Container Registry AcrPush Push and pull container images AI Search Service Search Index Data Contributor Read and write index data Storage Account Storage Blob Data Contributor Read and write blob data Cosmos DB Cosmos DB Built-in Data Contributor Read and write documents in Cosmos DB <p>Required Permissions (for deployment):</p> Resource Role Description App Configuration App Configuration Data Reader Read config Container Registry AcrPush Push images Container App Azure Container Apps Contributor Manage Container Apps <p>Deployment steps</p> <p>Make sure you're logged in to Azure before anything else:</p> <pre><code>az login\n</code></pre> <p>Clone this repository.</p> <p>If you used <code>azd provision</code></p> <p>Just run:</p> <pre><code>azd env refresh\nazd deploy \n</code></pre> <p>Make sure you use the same subscription, resource group, environment name, and location from <code>azd provision</code>.</p> <p>If you did not use <code>azd provision</code></p> <p>You need to set the App Configuration endpoint and run the deploy script.</p> <p>Bash (Linux/macOS):</p> <pre><code>export APP_CONFIG_ENDPOINT=\"https://&lt;your-app-config-name&gt;.azconfig.io\"\n./scripts/deploy.sh\n</code></pre> <p>PowerShell (Windows):</p> <pre><code>$env:APP_CONFIG_ENDPOINT = \"https://&lt;your-app-config-name&gt;.azconfig.io\"\n.\\scripts\\deploy.ps1\n</code></pre>"},{"location":"services_ingestion/#observability","title":"Observability","text":"<p>Monitor ingestion job execution and performance using Application Insights. The following query retrieves detailed metrics for completed ingestion runs, including indexing and purging operations.</p> <p>Application Insights Query</p> <p>Navigate to your Application Insights resource in the Azure Portal, go to Logs, and run the following query:</p> <pre><code>let Logs = union isfuzzy=true traces, AppTraces;\nLogs\n| where message contains \"RUN-COMPLETE\"\n| extend payload = parse_json(extract('\\\\{.*', 0, message))\n| where tostring(payload.event) == \"RUN-COMPLETE\"\n| extend indexerType = extract('\\\\[([^\\\\]]+)\\\\]', 1, message)\n| project timestamp,\n          indexerType,\n          runId = tostring(payload.runId),\n          status = tostring(payload.status),\n          collectionsSeen = toint(payload.collectionsSeen),\n          // Indexer columns (work on items)\n          itemsDiscovered = toint(payload.itemsDiscovered),\n          itemsIndexed = toint(payload.itemsIndexed),\n          itemsFailed = toint(payload.itemsFailed),\n          // Purger columns (work on chunks)\n          chunksChecked = toint(payload.chunksChecked),\n          chunksDeleted = toint(payload.chunksDeleted),\n          chunksFailedDelete = toint(payload.chunksFailedDelete),\n          // Common\n          durationSeconds = todouble(payload.durationSeconds)\n| order by timestamp desc\n</code></pre> <p>Query Fields</p> <p>This query returns the following metrics for each ingestion run:</p> Column Description <code>timestamp</code> When the job completed <code>indexerType</code> Type of indexer (e.g., Blob, SharePoint, NL2SQL) <code>runId</code> Unique identifier for the run <code>status</code> Job completion status <code>collectionsSeen</code> Number of collections processed <code>itemsDiscovered</code> Total items found during scan <code>itemsIndexed</code> Items successfully indexed <code>itemsFailed</code> Items that failed to index <code>chunksChecked</code> Chunks verified during purge <code>chunksScanned</code> Total chunks scanned <code>chunksDeleted</code> Chunks removed from index <code>chunksFailedDelete</code> Chunks that failed deletion <code>searchPages</code> Number of search result pages processed <code>durationSeconds</code> Total execution time in seconds"},{"location":"services_orchestrator/","title":"\ud83c\udfaf Orchestrator","text":"<p>The Orchestrator is the core engine of GPT-RAG, managing multi-agent workflows and retrieval operations using Semantic Kernel and Azure AI services. GitHub Repository.</p>"},{"location":"services_orchestrator/#key-features","title":"Key Features","text":"<ul> <li>Multi-Agent Workflows: Coordinates multiple AI agents for complex tasks</li> <li>Context Retrieval: Intelligent retrieval from Azure AI Search</li> <li>Semantic Kernel Integration: Built on Microsoft's Semantic Kernel framework</li> <li>Extensible Architecture: Easy to customize and extend</li> </ul>"},{"location":"services_orchestrator/#visual-guide","title":"Visual Guide","text":"<p>New to the Orchestrator? Check out our Orchestrator Visual Guide for a visual walkthrough of the architecture and key components.</p>"},{"location":"services_orchestrator/#repository","title":"Repository","text":"<p>\ud83d\udd17 GitHub Repository</p>"},{"location":"services_orchestrator_visual_guide/","title":"Orchestrator: Start Your Code Reading with Visuals","text":"<p>A picture is worth a thousand words. Yet many engineers write another thousand words instead of drawing a single useful diagram. Let\u2019s reverse this evolution \u2014 with visuals.</p> <p>Starting with diagrams \u2014 with a bit of simplification and abstraction \u2014 can significantly accelerate the comprehension of complex codebases. This is especially true when the data flow spans multiple execution environments (container app, Microsoft Foundry, Azure cloud resources), where the initial orientation can otherwise be challenging.</p>"},{"location":"services_orchestrator_visual_guide/#why-this-article-exists","title":"Why This Article Exists","text":"<p>When I started reading the code, I struggled:</p> <ul> <li>Where is the entry point?</li> <li>What calls what?</li> <li>What is the role of the Orchestrator in the data flow?</li> </ul> <p>If you have ever felt dazed and confused by a codebase with many layers of abstraction, this is for you.</p> <p>If you prefer talking for hours about a diagram instead of drawing it, just leave.</p>"},{"location":"services_orchestrator_visual_guide/#core-architecture-flow","title":"Core Architecture &amp; Flow","text":"<p>The orchestrator's entry point is in src/main.py: <code>orchestrator_endpoint()</code>. In what follows we will consider the Single-Agent RAG Strategy. <pre><code>@app.post(\n    \"/orchestrator\",\n    dependencies=[Depends(validate_auth)], \n    summary=\"Ask orchestrator a question\",\n    response_description=\"Returns the orchestrator\u2019s response in real time, streamed via SSE.\",\n    responses=ORCHESTRATOR_RESPONSES\n)\nasync def orchestrator_endpoint(\n    body: OrchestratorRequest,\n    x_api_key: Optional[str] = Header(None, alias=\"X-API-KEY\"),\n    dapr_api_token: Optional[str] = Header(None, alias=\"dapr-api-token\"),\n):\n</code></pre></p>"},{"location":"services_orchestrator_visual_guide/#essential-tasks-of-the-orchestrator","title":"Essential Tasks of the Orchestrator","text":"<p>The <code>Orchestrator</code> class serves as the conversation state manager and strategy coordinator. Its core responsibilities are:</p> <ol> <li>Conversation Lifecycle Management: Creates, loads, and persists conversation documents in the CosmosDB.</li> <li>Strategy Delegation: Routes processing to appropriate agent strategies via factory pattern (<code>AgentStrategyFactory</code>).</li> <li>State Coordination: Ensures conversation state is properly synchronized between database and strategy</li> <li>Response Streaming: Coordinates real-time response delivery while maintaining state consistency</li> </ol>"},{"location":"services_orchestrator_visual_guide/#single-agent-strategy","title":"Single Agent Strategy","text":"<p>This section explores how the Single-Agent RAG Strategy orchestrates the entire request-response lifecycle, from receiving a user's question to delivering a grounded, streamed answer. The diagrams below illustrate the conversation lifecycle, state management, and the interaction between the Orchestrator container app and Microsoft Foundry services.</p> <p></p> <p>You will have noticed the use of the Factory Design Pattern (<code>AgentStrategyFactory</code>) for the various Strategies, ensuring that all of them comply with the same <code>BaseAgentStrategy</code> interface. For the sake of clarity, I have abstracted away the different roles of the <code>Orchestrator</code> class and the <code>Orchestrator</code> object.</p> <pre><code>------------------------------------------------------------------\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                         Orchestrator                           \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502  - conversation_id: str                                  \u2502  \u2502\n\u2502  \u2502  - database_client: CosmosDBClient     &lt;&lt;reference&gt;&gt;     \u2502  \u2502\n\u2502  \u2502  - agentic_strategy: BaseAgentStrategy \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502  \u2502\n\u2502  \u2502                                                       \u2502  \u2502  \u2502\n\u2502  \u2502  + create()                                           \u2502  \u2502  \u2502\n\u2502  \u2502  + stream_response()                                  \u2502  \u2502  \u2502\n\u2502  \u2502  + save_feedback()                                    \u2502  \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                      \u2502                                    \u2502\n                      \u2502 uses (delegation)                  \u2502 \n                      \u25bc                                    \u2502\n           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                        \u2502  \n           \u2502 AgentStrategyFactory \u2502 &lt;&lt;factory&gt;&gt;            \u2502\n           \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524                        \u2502\n           \u2502 + get_strategy(key)  \u2502                        \u2502\n           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                        \u2502\n                      \u2502                                    \u2502\n                      \u2502 instantiates                       \u2502\n                      \u25bc                                    \u2502\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                    \u2502\n        \u2502    BaseAgentStrategy        \u2502 &lt;&lt;abstract&gt;&gt; \u25c4\u2500\u2500\u2500\u2500\u2500\u2518\n        \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n        \u2502 # strategy_type             \u2502\n        \u2502 # conversation: Dict        \u2502\n        \u2502 # user_context: Dict        \u2502\n        \u2502 # credential                \u2502\n        \u2502 # project_client            \u2502\n        \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n        \u2502 + initiate_agent_flow()*    \u2502 * = abstract method\n        \u2502 # _read_prompt()            \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                       \u2502\n                       \u2502 inheritance (IS-A)\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502             \u2502                         \u2502\n         \u25bc             \u25bc                         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 SingleAgent    \u2502 \u2502  NL2SQL      \u2502  \u2502  McpStrategy    \u2502\n\u2502 RAGStrategy    \u2502 \u2502  Strategy    \u2502  \u2502                 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 - tools_list   \u2502 \u2502 - nl2sql     \u2502  \u2502 - kernel        \u2502\n\u2502 - ai_search    \u2502 \u2502   _plugin    \u2502  \u2502 - agent         \u2502\n\u2502 - event_handler\u2502 \u2502 - terminator \u2502  \u2502                 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 + initiate_    \u2502 \u2502 + initiate_  \u2502  \u2502 + initiate_     \u2502\n\u2502   agent_flow() \u2502 \u2502   agent_flow \u2502  \u2502   agent_flow()  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n------------------------------------------------------------------\n</code></pre> <p>The entry point for the selected Strategy is the method <code>agentic_strategy.initiate_agent_flow()</code>.</p> <p>The strategy object instantiated from the <code>SingleAgentRAGStrategy</code> class runs in the container app and controls the sequence of activities behind the Microsoft Foundry wall. It uses the <code>project_client</code> object as a local proxy (think of it as a remote TV control) to orchestrate operations. The strategy object doesn't handle grounding or LLM calls directly\u2014these are delegated to the Microsoft Foundry agent where the entire RAG pattern is executed.</p> <p>The strategy object <code>SingleAgentRAGStrategy</code></p> <ul> <li> <p>creates a new agent by specifying instructions and a toolbox,</p> </li> <li> <p>retrieves the <code>Thread</code> object based on the thread_id which was retrieved from the CosmosDB,</p> </li> <li> <p>creates a new message from the user's Ask and attaches it to the Thread object,</p> </li> <li> <p>finally calls the project_client.agents.runs.stream() which triggers the RAG pipeline inside of the Microsoft Foundry realm.</p> </li> </ul> <p>Note that <code>Thread</code> objects keep the entire history of conversations. There are two levels of history persistence: one in CosmosDB and another in Thread objects.</p> <p><code>Orchestrator</code> keeps a history (using CosmosDB) identified by <code>conversation_id</code> which arrives in the HTTP Request payload. One of the attributes stored in the CosmosDB is <code>thread_id</code> which points to the <code>Thread</code> object which resides inside of the Microsoft Foundry. Microsoft Foundry maintains its own internal persistency in the Thread objects.</p> <p>The strategy object triggers the RAG pipeline execution inside of Microsoft Foundry with the proxy <code>project_client</code>: <pre><code>project_client.agents.runs.stream(\n    thread_id=thread.id,\n    agent_id=agent.id,\n    ...\n)\n</code></pre> Here is what it does:</p> <ul> <li> <p>Takes the user's Ask.</p> </li> <li> <p>Queries your Azure AI Search index using the <code>AzureAISearchTool</code>.</p> </li> <li> <p>Retrieves relevant document Chunks.</p> </li> <li> <p>Creates the Prompt.</p> </li> <li> <p>Previously retrieved Chunks are included into the Prompt to ground the Response.</p> </li> <li> <p>Prompt is fed into LLM which generates Response.</p> </li> <li> <p>The Response is enhanced by citations and references to the grounding documents.</p> </li> </ul>"},{"location":"services_orchestrator_visual_guide/#single-agent-strategy-internal-flow","title":"Single Agent Strategy Internal Flow","text":"<p>The sequence diagram above is intended to illustrate the core concepts and design patterns present in the codebase. The visualization deliberately simplifies reality through abstraction and by omitting less relevant details.</p>"},{"location":"services_orchestrator_visual_guide/#links-to-the-code","title":"Links to the code","text":"Concept File Notes Orchestrator entry <code>main.py</code> FastAPI route + request handling Orchestrator implementation <code>orchestration/orchestrator.py</code> Maintains Conversation History + runs streaming pipeline Strategy factory <code>strategies/agent_strategy_factory.py</code> Selects the execution strategy Single-Agent RAG Strategy <code>strategies/single_agent_rag_strategy.py</code> Implements flow to Azure Microsoft Foundry"},{"location":"team/","title":"Meet the Team","text":"Core Team Leads the direction, development, and continuous evolution of GPT-RAG. <sub>Paulo Lacerda</sub> <sub>Project Lead</sub> <sub>Vlad Borys</sub> <sub>Pull Request Reviewer</sub> Engineering Advisor Provides technical guidance and ensures architectural alignment with best practices. <sub>Pablo Castro</sub> <sub>Engineering Advisor</sub> Founders The visionaries who laid the foundation and shaped GPT-RAG from the ground up. <sub>Paulo Lacerda</sub> <sub>Gonzalo Becerra</sub> <sub>Martin Sciarrilo</sub> Contributors Every contribution matters \u2014 code, documentation, ideas, or feedback. GPT-RAG Orchestrator Ingestion UI MCP Champions Community advocates helping users succeed, sharing knowledge, and growing the GPT-RAG ecosystem. <sub>Ramesh Jajula</sub> <sub>Vinod Chekkala</sub> <sub>Saurabh Singh</sub> <sub>Varun Nambia</sub> <p>Every contribution matters! Whether it's code, docs, ideas, or feedback \u2014 you help make GPT-RAG better.</p> <p>\ud83d\udc49 Learn how to contribute</p>"},{"location":"whatisnew/","title":"What's New","text":"<p>\ud83d\udccc Check out what's coming next  (Azure org only)</p>"},{"location":"whatisnew/#october-2025","title":"October 2025","text":"<p>Release 2.2.0 - Agentic Retrieval and Network Flexibility</p> <p>This release introduces major enhancements to support more flexible and enterprise-ready deployments.</p> <p>Bring Your Own VNet   Enables organizations to deploy GPT-RAG within their existing virtual network, maintaining full control over network boundaries, DNS, and routing policies.   #370</p> <p>Agentic Retrieval   Adds intelligent, agent-driven retrieval orchestration that dynamically selects and combines information sources for more grounded and context-aware responses.   #359</p>"},{"location":"whatisnew/#september-2025","title":"September 2025","text":"<p>Release 2.1.0 - User Feedback Loop</p> <p>Introduces a mechanism for end-users to provide thumbs-up or thumbs-down feedback on assistant responses, storing these signals alongside conversation history to continuously improve response quality.</p> <ul> <li>How to configure it: User Feedback Configuration</li> <li>Demo video:</li> </ul>"}]}